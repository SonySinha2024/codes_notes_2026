What is an agent?¶
An agent consists of three components: a large language model (LLM), a set of tools it can use, and a prompt that provides instructions.

The LLM operates in a loop. In each iteration, it selects a tool to invoke, provides input, receives the result (an observation), 
and uses that observation to inform the next action. The loop continues until a stopping 
condition is met — typically when the agent has gathered enough information to respond to the user.

Visualize an agent graph¶
Use the following tool to visualize the graph generated by create_react_agent and to view an outline of the corresponding code. 
It allows you to explore the infrastructure of the agent as defined by the presence of:
  
tools: A list of tools (functions, APIs, or other callable objects) that the agent can use to perform tasks.
pre_model_hook: A function that is called before the model is invoked. It can be used to condense messages or perform other preprocessing tasks.
post_model_hook: A function that is called after the model is invoked. It can be used to implement guardrails, human-in-the-loop flows,
 or other postprocessing tasks.
response_format: A data structure used to constrain the type of the final output, e.g., a pydantic BaseModel.

create_react_agent	Creates an agent graph that calls tools in a loop until a stopping condition is met
create_react_agent(
    model: Union[str, LanguageModelLike],
    tools: Union[
        Sequence[Union[BaseTool, Callable, dict[str, Any]]],
        ToolNode,
    ],
    *,
    prompt: Optional[Prompt] = None,
    response_format: Optional[
        Union[
            StructuredResponseSchema,
            tuple[str, StructuredResponseSchema],
        ]
    ] = None,
    pre_model_hook: Optional[RunnableLike] = None,
    post_model_hook: Optional[RunnableLike] = None,
    state_schema: Optional[StateSchemaType] = None,
    config_schema: Optional[Type[Any]] = None,
    checkpointer: Optional[Checkpointer] = None,
    store: Optional[BaseStore] = None,
    interrupt_before: Optional[list[str]] = None,
    interrupt_after: Optional[list[str]] = None,
    debug: bool = False,
    version: Literal["v1", "v2"] = "v2",
    name: Optional[str] = None
) -> CompiledGraph



Pregel manages the runtime behavior for LangGraph applications.

Overview¶
Pregel combines actors and channels into a single application. Actors read data from channels and write data to channels.
 Pregel organizes the execution of the application into multiple steps, 
following the Pregel Algorithm/Bulk Synchronous Parallel model.

Each step consists of three phases:

Plan: Determine which actors to execute in this step. For example, in the first step, select the actors that subscribe to the special input channels; in subsequent steps, select the actors that subscribe to channels updated in the previous step.
Execution: Execute all selected actors in parallel, until all complete, or one fails, or a timeout is reached. During this phase, channel updates are invisible to actors until the next step.
Update: Update the channels with the values written by the actors in this step.
Repeat until no actors are selected for execution, or a maximum number of steps is reached.

Actors¶
An actor is a PregelNode. It subscribes to channels, reads data from them, and writes data to them. It can be thought of as an actor in the Pregel algorithm. PregelNodes implement LangChain's Runnable interface.

Channels¶
Channels are used to communicate between actors (PregelNodes). Each channel has a value type, an update type, and an update function – which takes a sequence of updates and modifies the stored value. Channels can be used to send data from one chain to another, or to send data from a chain to itself in a future step. LangGraph provides a number of built-in channels:

Basic channels: LastValue and Topic¶
LastValue: The default channel, stores the last value sent to the channel, useful for input and output values, or for sending data from one step to the next
Topic: A configurable PubSub Topic, useful for sending multiple values between actors, or for accumulating output. Can be configured to deduplicate values, and/or to accumulate values over the course of multiple steps.
Advanced channels: Context and BinaryOperatorAggregate¶
Context: exposes the value of a context manager, managing its lifecycle. Useful for accessing external resources that require setup and/or teardown. eg. client = Context(httpx.Client)
BinaryOperatorAggregate: stores a persistent value, updated by applying a binary operator to the current value and each update sent to the channel, useful for computing aggregates over multiple steps. eg. total = BinaryOperatorAggregate(int, operator.add)
Examples¶
Most users will interact with Pregel via a StateGraph (Graph API) or via an entrypoint (Functional API).

However, for advanced use cases, Pregel can be used directly. If you're not sure whether you need to use Pregel directly, then the answer is probably no – you should use the Graph API or Functional API inste


Running agents¶
Agents support both synchronous and asynchronous execution using either .invoke() / await .ainvoke() for full responses,
 or .stream() / .astream() for incremental streaming output.

To stream agent progress, use the stream() or astream() methods with stream_mode="updates". This emits an event after every agent step.

To stream tokens as they are produced by the LLM, use stream_mode="messages"

interrupt¶
The interrupt function in LangGraph enables human-in-the-loop workflows by pausing the graph at a specific node, 
presenting information to a human, and resuming the graph with their input. It's useful for tasks like approvals, edits, or gathering additional context.

The graph is resumed using a Command object that provides the human's response.

Breakpoints pause graph execution at defined points and let you step through each stage. They use LangGraph's persistence layer,
 which saves the graph state after each step.

With breakpoints, you can inspect the graph's state and node inputs at any point. Execution pauses indefinitely 
until you resume, as the checkpointer preserves the state.
=========================

Model Context Protocol (MCP) is an open protocol that standardizes how applications provide tools and context to language models.
 LangGraph agents can use tools defined on MCP servers through the langchain-mcp-adapters library.

The langchain-mcp-adapters package enables agents to use tools defined across one or more MCP servers.

mcp servers provides context, tools, prompts to clients,
each server can have many tools
mcp client maintain 1:1 connections with servers inside the host app

from langchain_mcp_adapters.client import MultiServerMCPClient
from langgraph.prebuilt import create_react_agent

client = MultiServerMCPClient(
    {
        "math": {
            "command": "python",
            # Replace with absolute path to your math_server.py file
            "args": ["/path/to/math_server.py"],
            "transport": "stdio",
        },
        "weather": {
            # Ensure your start your weather server on port 8000
            "url": "http://localhost:8000/mcp",
            "transport": "streamable_http",
        }
    }
)
tools = await client.get_tools()
agent = create_react_agent(
    "anthropic:claude-3-7-sonnet-latest",
    tools
)
math_response = await agent.ainvoke(
    {"messages": [{"role": "user", "content": "what's (3 + 5) x 12?"}]}
)
weather_response = await agent.ainvoke(
    {"messages": [{"role": "user", "content": "what is the weather in nyc?"}]}
)

MCP is an open protocol that standardizes how applications provide context to LLMs

from mcp.server.fastmcp import FastMCP

mcp = FastMCP("Weather")

@mcp.tool()
async def get_weather(location: str) -> str:
    """Get weather for location."""
    return "It's always sunny in New York"

if __name__ == "__main__":
    mcp.run(transport="streamable-http")

MCP helps you build agents and complex workflows on top of LLMs. LLMs frequently need to integrate with data and tools, and MCP provides:

A growing list of pre-built integrations that your LLM can directly plug into
The flexibility to switch between LLM providers and vendors
Best practices for securing your data within your infrastructure

General architecture
At its core, MCP follows a client-server architecture where a host application can connect to multiple servers:

MCP Hosts: Programs like Claude Desktop, IDEs, or AI tools that want to access data through MCP
MCP Clients: Protocol clients that maintain 1:1 connections with servers
MCP Servers: Lightweight programs that each expose specific capabilities through the standardized Model Context Protocol
Local Data Sources: Your computer’s files, databases, and services that MCP servers can securely access
Remote Services: External systems available over the internet (e.g., through APIs) that MCP servers can connect to


The Model Context Protocol (MCP) is built on a flexible, extensible architecture that enables 
seamless communication between LLM applications and integrations. This document covers the core architectural components and concepts.

​
Overview
MCP follows a client-server architecture where:

Hosts are LLM applications (like Claude Desktop or IDEs) that initiate connections
Clients maintain 1:1 connections with servers, inside the host application
Servers provide context, tools, and prompts to clients



Protocol layer
The protocol layer handles message framing, request/response linking, and high-level communication patterns.
Key classes include:

Protocol
Client
Server

The transport layer handles the actual communication between clients and servers. MCP supports multiple transport mechanisms:

Stdio transport

Uses standard input/output for communication
Ideal for local processes
Streamable HTTP transport

Uses HTTP with optional Server-Sent Events for streaming
HTTP POST for client-to-server messages
All transports use JSON-RPC 2.0 to exchange messages. See the specification for detailed information about the Model Context Protocol message format

https://modelcontextprotocol.io/docs/concepts/architecture#python

In LangGraph:

Short-term memory is also referred to as thread-level memory.
Long-term memory is also called cross-thread memory.
A thread represents a sequence of related runs grouped by the same thread_id.


=================
Prompts refer to the messages that are passed into the language model.

Prompt Templates refer to a way of formatting information to get that prompt to hold the information that you want. Prompt templates can include variables 
for few shot examples, outside context, or any other external data that is needed in your prompt.

A chat model's context window refers to the maximum size of the input sequence the model can process at one time. 
In conversational applications, this is especially important because the context window determines how much information 
the model can "remember" throughout a conversation. 

LangChain chat models implement the BaseChatModel interface. 
Because BaseChatModel also implements the Runnable Interface, 
chat models support a standard streaming interface, async programming, optimized batching, and more.

Many of the key methods of chat models operate on messages as input and return messages as output.

Chat models offer a standard set of parameters that can be used to configure the model. 
These parameters are typically used to control the behavior of the model, 
such as the temperature of the output, the maximum number of tokens in the response, and the maximum time to wait for a response. 

The key methods of a chat model are:

invoke: The primary method for interacting with a chat model. It takes a list of messages as input and returns a list of messages as output.
stream: A method that allows you to stream the output of a chat model as it is generated.
batch: A method that allows you to batch multiple requests to a chat model together for more efficient processing.
bind_tools: A method that allows you to bind a tool to a chat model for use in the model's execution context.
with_structured_output: A wrapper around the invoke method for models that natively support structured output.


Rate-limiting
Many chat model providers impose a limit on the number of requests that can be made in a given time period.

If you hit a rate limit, you will typically receive a rate limit error response from the provider, and will need to wait before making more requests.

You have a few options to deal with rate limits:

Try to avoid hitting rate limits by spacing out requests: Chat models accept a rate_limiter parameter that can be provided during initialization.
 This parameter is used to control the rate at which requests are made to the model provider. Spacing out the requests to 
a given model is a particularly useful strategy when benchmarking models to evaluate their performance. 


 semantic caching, where you cache responses based on the meaning of the input rather than the exact input itself.

Messages are the unit of communication in chat models.
A message typically consists of the following pieces of information:

Role: The role of the message (e.g., "user", "assistant").
Content: The content of the message (e.g., text, multimodal data).
Additional metadata: id, name, token usage and other model-specific metadata.

It is common to stream responses for the chat model as they are being generated, so the user can see the
 response in real-time instead of waiting for the entire response to be generated before displaying it.
AIMessageChunks support the + operator to merge them into a single AIMessage. This is useful when you want to display the final response to the user.
for chunk in model.stream([HumanMessage("what color is the sky?")]):
    print(chunk)

A token is the basic unit that a language model reads, processes, and generates. 
The way the model tokenizes the input depends on its tokenizer algorithm, which converts the input into tokens. Similarly, 
the model’s output comes as a stream of tokens, which is then decoded back into human-readable text.


A Run is a span representing a single unit of work or operation within your LLM application. This could be anything
 from single call to an LLM or chain, to a prompt formatting call, to a runnable lambda invocation. 

Tags are collections of strings that can be attached to runs. They are used to categorize runs and make it easier to search for them in the LangSmith UI.

A Trace is a collection of runs that are related to a single operation. For example, 
if you have a user request that triggers a chain, and that chain makes
 a call to an LLM, then to an output parser, and so on, all of these runs would be part of the same trace


Tool Creation: Use the @tool decorator to create a tool. A tool is an association between a function and its schema.
Tool Binding: The tool needs to be connected to a model that supports tool calling. This gives the model awareness of the tool and 
the associated input schema required by the tool.
Tool Calling: When appropriate, the model can decide to call a tool and ensure its response conforms to the tool's input schema.
Tool Execution: The tool can be executed using the arguments provided by the model.

from langchain_core.tools import tool

@tool
def multiply(a: int, b: int) -> int:
    """Multiply a and b."""
    return a * b
llm_with_tools = tool_calling_model.bind_tools([multiply])


A key principle of tool calling is that the model decides when to use a tool based on the input's relevance. 
The model doesn't always need to call a tool. For example, given an unrelated input, the model would not call the tool:

result = llm_with_tools.invoke("Hello world!")

Tools implement the Runnable interface, which means that they can be invoked (e.g., tool.invoke(args)) directly.

LangGraph offers pre-built components (e.g., ToolNode) that will often invoke the tool in behalf of the user

but it does not remember the context of previous interactions. This limits its ability to have coherent, multi-turn conversations.

LangGraph solves this problem through persistent checkpointing. If you provide a checkpointer when compiling the graph and a thread_id 
when calling your graph, LangGraph automatically saves the state after each step. When you invoke the graph again using 
the same thread_id, the graph loads its saved state, allowing the chatbot to pick up where it left off.

LangGraph's persistence layer supports human-in-the-loop workflows, allowing execution to pause and resume based on user feedback.
 The primary interface to this functionality is the interrupt function. Calling interrupt inside a node will pause execution.
 Execution can be resumed, together with new input from a human, by passing in a Command. 

memory = MemorySaver()

graph = graph_builder.compile(checkpointer=memory)



==============================================================================
LangSmith
A developer platform that lets you debug, test, evaluate, and monitor LLM applications.

langserve
A package to deploy LangChain chains as REST APIs. Makes it easy to get a production ready API up and running.
==================
popular frameworks for building Agentic AI Applications — LangChain and LangGraph.

There are two methods for working with LangChain: as a sequential chain of predefined commands or using LangChain agents.
 Each approach is different in the way it handles tools and orchestration.
 A chain follows a predefined linear workflow while an agent acts as a coordinator that can make more dynamic (non linear) decisions.

Chains: A sequence of steps that can include calls to an llm, agent, tool, external data source, procedural code, and more. 
Chains can branch, meaning a single chain to split into multiple paths based on logical conditions.
Agents or Language Models: A Language Model has the ability to generate responses in natural language
. But the Agent uses a language model plus added capabilities to reason, call tools, and repeat the process of calling tools in case there are any failures.
Tools: Code based functions that can be called in the chain or invoked by an agent to interact with external systems.
Prompts: This can include a system prompt that instructs the model how to complete a task and what tools are available, 
information injected from external data sources that provided the model more context, and the user prompt or task for the model to complete.

conditional, stateful LLM agents using LangGraph.
LangGraph was designed to handle more complex conditional logic and feedback loops compared to LangChain.

it orchestrates workflows like a graph
Graphs: A flexible way of organizing a workflow that can include calls to an llm, tool, external data source, procedural code, and more.
 LangGraph supports cyclical graphs as well; which means you can create loops and feedback mechanisms so nodes can be revisited multiple times.
Nodes: Represent steps in the workflow, such as an LLM query, an API call, or tool execution.
Edges and Conditional Edges: Edges define the flow of information by connecting the output of one node as the input to the next.
 A conditional edge defines the flow of information from one node to another if a certain condition is met. 
Developers can custom define these conditions.
State: State is the current status of the application as information flows through the graph. 
It is a developer defined mutable TypedDict object that contains all the relevant information for the current execution of the graph.
 LangGraph automatically handles the updating of state at each node as information flows through the graph.
Agents or Language Models: Language models within a graph are solely responsible for generating a text response to an input.
 The agent capability leverages a language model but enables the graph to have multiple nodes representing different components 
of the agent (such as reasoning, tool selection, and execution of a tool). The agent can make decisions about which path to take in the graph, 
 update the state of the graph, and perform more tasks than just text generation.


======================================
Redis (Remote Dictionary Server) is an open-source in-memory storage, used as a distributed, in-memory key–value database, 
cache and message broker, with optional durability. Because it holds all data in memory and because of its design, 
Redis offers low-latency reads and writes, making it particularly suitable for use cases that require a cache. 
Redis is the most popular NoSQL database, and one of the most popular databases overall.

Google Cloud Firestore is a serverless document-oriented database that scales to meet any demand. 
Extend your database application to build AI-powered experiences leveraging Firestore's Langchain integrations.

This notebook goes over how to use Google Cloud Firestore to store chat message history with the FirestoreChatMessageHistory class.
To initialize the FirestoreChatMessageHistory class you need to provide only 3 things:

session_id - A unique identifier string that specifies an id for the session.
collection : The single /-delimited path to a Firestore collection.
from langchain_google_firestore import FirestoreChatMessageHistory

chat_history = FirestoreChatMessageHistory(
    session_id="user-session-id", collection="HistoryMessages"
)

The Cache wrapper allows for Redis to be used as a remote, low-latency, in-memory cache for LLM prompts and responses.

from langchain.cache import RedisCache

Semantic caching allows users to retrieve cached prompts based on semantic similarity between the user input and previously cached results. 
The vectorstore wrapper turns Redis into a low-latency vector database for semantic search or LLM content retrieval.
The Redis vector store retriever wrapper generalizes the vectorstore class to perform low-latency document retrieval. 
To create the retriever, simply call .as_retriever() on the base vectorstore class

The Redis vector store retriever wrapper generalizes the vectorstore class to perform low-latency document retrieval. 
To create the retriever, simply call .as_retriever() on the base vectorstore class


# Create a prompt template
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a helpful AI assistant."),
        MessagesPlaceholder(variable_name="history"),
        ("human", "{input}"),
    ]
)

# Initialize the language model
llm = ChatOpenAI()

# Create the conversational chain
chain = prompt | llm


# Function to get or create a RedisChatMessageHistory instance
def get_redis_history(session_id: str) -> BaseChatMessageHistory:
    return RedisChatMessageHistory(session_id, redis_url=REDIS_URL)


# Create a runnable with message history
chain_with_history = RunnableWithMessageHistory(
    chain, get_redis_history, input_messages_key="input", history_messages_key="history"
)

# Use the chain in a conversation
response1 = chain_with_history.invoke(
    {"input": "Hi, my name is Alice."},
    config={"configurable": {"session_id": "alice_123"}},
)
print("AI Response 1:", response1.content)

response2 = chain_with_history.invoke(
    {"input": "What's my name?"}, config={"configurable": {"session_id": "alice_123"}}
)
print("AI Response 2:", response2.content)

Any of the methods that are used to execute the runnable (e.g., invoke, batch, stream, astream_events) accept 
a second argument called RunnableConfig (API Reference). 
This argument is a dictionary that contains configuration for the Runnable that will be used at run time during the execution of the runnable

There are two main patterns by which new Runnables are created:

Declaratively using LangChain Expression Language (LCEL):

chain = prompt | chat_model | output_parser

Using a custom Runnable (e.g., RunnableLambda) or using the @tool decorator:

def foo(input):
    # Note that .invoke() is used directly here
    return bar_runnable.invoke(input)
foo_runnable = RunnableLambda(foo)

The LangChain Expression Language (LCEL) takes a declarative approach to building new Runnables from existing Runnables.

This means that you describe what should happen, rather than how it should happen, allowing LangChain to optimize the run-time execution of the chains.

We often refer to a Runnable created using LCEL as a "chain". It's important to remember that a "chain" is Runnable and it implements
 the full Runnable Interface.


=============
the tool abstraction in LangChain associates a Python function with a schema that defines the function's name, description and expected arguments.

Tools can be passed to chat models that support tool calling allowing the model to request the execution of a specific function with specific inputs.

Tools are a way to encapsulate a function and its schema in a way that can be passed to a chat model.
Create tools using the @tool decorator, which simplifies the process of tool creation, supporting the following:
Automatically infer the tool's name, description and expected arguments, while also supporting customization.
Defining tools that return artifacts (e.g. images, dataframes, etc.)

The tool interface is defined in the BaseTool class which is a subclass of the Runnable Interface.

The key attributes that correspond to the tool's schema:

name: The name of the tool.
description: A description of what the tool does.
args: Property that returns the JSON schema for the tool's arguments.
The key methods to execute the function associated with the tool:

invoke: Invokes the tool with the given arguments.
ainvoke: Invokes the tool with the given arguments, asynchronously. Used for async programming with Langchain.

The recommended way to create tools is using the @tool decorator. This decorator is designed to simplify the process
 of tool creation and should be used in most cases. After defining a function, 
you can decorate it with @tool to create a tool that implements the Tool Interface.

from langchain_core.tools import tool

@tool
def multiply(a: int, b: int) -> int:
   """Multiply two numbers."""
   return a * b

Once you have defined a tool, you can use it directly by calling the function. For example, to use the multiply tool defined above:

multiply.invoke({"a": 2, "b": 3})

Inspect
You can also inspect the tool's schema and other properties:

print(multiply.name) # multiply
print(multiply.description) # Multiply two numbers.
print(multiply.args) 
# {
# 'type': 'object', 
# 'properties': {'a': {'type': 'integer'}, 'b': {'type': 'integer'}}, 
# 'required': ['a', 'b']
# }

The @tool decorator offers additional options to configure the schema of the tool 
(e.g., modify name, description or parse the function's doc-string to infer the schema).


The newest generation of chat models offer additional capabilities:

Tool calling: Many popular chat models offer a native tool calling API. This API allows developers to build 
rich applications that enable LLMs to interact with external services, APIs, and databases. 
Tool calling can also be used to extract structured information from unstructured data and perform various other tasks.
Structured output: A technique to make a chat model respond in a structured format, such as JSON that matches a given schema.
Multimodality: The ability to work with data other than text; for example, images, audio, and video.


LangChain chat models implement the BaseChatModel interface. Because BaseChatModel also implements the Runnable Interface,
 chat models support a standard streaming interface, async programming, optimized batching, 


In LangGraph, the state handles memory by keeping track of defined variables at every point in time. 
State can include things like conversation history, steps of a plan, the output of a language model’s previous response, and more.
 It can be passed from one node to the next so that each node has access to what the current state of the system is.
 However, long term persistent memory across sessions is not available as a direct feature of the framework. 
To implement this, developers could include nodes 
responsible to store memories and other variables in an external database to be retrieved later.


LangChain can handle complex retrieval and generation workflows and has a more established set of tools to help developers
 integrate RAG into their application. For instance LangChain offers document loading, text parsing, embedding creation, 
vector storage, and retrieval capabilities out of 
the box by using langchain.document_loaders, langchain.embeddings, and langchain.vectorstores directly.

Use LangChain Only When:

You need to quickly prototype or develop AI workflows that either involve sequential tasks (such as such as document retrieval, text generation, 
or summarization) that follow a predefined linear pattern. Or you want to leverage AI agent patterns that can dynamically make decisions,
 but you don’t need granular control over a complex workflow.

Use LangGraph Only When:

Your use case requires non-linear workflows where multiple components interact dynamically such as workflows
 that depend on conditions, need complex branching logic, error handling, or parallelism. 
You are willing to build custom implementations for the components that are not abstracted for you like in LangChain.

Using LangChain and LanGraph Together When:

You enjoy the pre-built extracted components of LangChain such as the out of the box RAG capabilities, 
memory functionality, etc. but also want to manage complex task flows using LangGraph’s non-linear orchestration. 
Using both frameworks together can be a powerful tool for extracting the best abilities from each.

The Runnable way defines a standard interface that allows a Runnable component to be:

Invoked: A single input is transformed into an output.
Batched: Multiple inputs are efficiently transformed into outputs.
Streamed: Outputs are streamed as they are produced.
Inspected: Schematic information about Runnable's input, output, and configuration can be accessed.
Composed: Multiple Runnables can be composed to work together using the LangChain Expression Language (LCEL) to create complex pipelines.

Runnable methods that result in the execution of the Runnable (e.g., invoke, batch, stream, astream_events) work with these input and output types.

invoke: Accepts an input and returns an output.
batch: Accepts a list of inputs and returns a list of outputs.
stream: Accepts an input and returns a generator that yields outputs.
=====================================

Beautiful Soup is a Python package for parsing HTML and XML documents (including having malformed markup,
 i.e. non-closed tags, so named after tag soup). It creates a parse tree for parsed pages that can be used to extract data from HTML,[3] 
   which is useful for web scraping.

Beautiful Soup offers fine-grained control over HTML content, enabling specific tag extraction, removal, and content cleaning.

It's suited for cases where you want to extract specific information and clean up the HTML content according to your needs.

For example, we can scrape text content within <p>, <li>, <div>, and <a> tags from the HTML content:

<p>: The paragraph tag. It defines a paragraph in HTML and is used to group together related sentences and/or phrases.

<li>: The list item tag. It is used within ordered (<ol>) and unordered (<ul>) lists to define individual items within the list.

<div>: The division tag. It is a block-level element used to group other inline or block-level elements.

<a>: The anchor tag. It is used to define hyperlinks.

from langchain_community.document_loaders import AsyncChromiumLoader
from langchain_community.document_transformers import BeautifulSoupTransformer

# Load HTML
loader = AsyncChromiumLoader(["https://www.wsj.com"])
html = loader.load()

LangChain provides a key-value store interface for storing and retrieving data.
The key-value store interface in LangChain is used primarily for:

Caching embeddings via CachedBackedEmbeddings to avoid recomputing embeddings for repeated queries or when re-indexing content.

Embedding models transform human language into a format that machines can understand and compare with speed and accuracy.
 These models take text as input and produce a fixed-length array of numbers, a numerical fingerprint of the text's semantic meaning. 
Embeddings allow search system to find relevant documents not just based on keyword matches, but on semantic understanding.

(1) Embed text as a vector: Embeddings transform text into a numerical vector representation.

(2) Measure similarity: Embedding vectors can be compared using simple mathematical operations

embeddings = embeddings_model.embed_documents(
    [
        "Hi there!",
        "Oh, hello!",
        "What's your name?",
        "My friends call me World",
        "Hello World!"
    ]
)
query_embedding = embeddings_model.embed_query("What is the meaning of life?")

Cosine Similarity: Measures the cosine of the angle between two vectors.
Euclidean Distance: Measures the straight-line distance between two points.
Dot Product: Measures the projection of one vector onto another.

A vector store stores embedded data and performs similarity search
from langchain_core.vectorstores import InMemoryVectorStore

vector_store = InMemoryVectorStore(embeddings)

Vector stores are specialized data stores that enable indexing and retrieving information based on vector representations.

These vectors, called embeddings, capture the semantic meaning of data that has been embedded.

Vector stores are frequently used to search over unstructured data, such as text, images, and audio, 
to retrieve relevant information based on semantic similarity rather than exact keyword matches.

Tools are utilities designed to be called by a model: their inputs are designed to be generated by models, 
and their outputs are designed to be passed back to models.

A toolkit is a collection of tools meant to be used together
=======================
AutoGen is an open-source framework developed by Microsoft that simplifies the orchestration, optimization, and automation 
of workflows involving large language models 
It enables the creation of multi-agent systems where agents can converse and collaborate to solve complex tasks.

Key Features of Microsoft's Project AutoGen

Customizable Agents: AutoGen allows developers to define specialized agents with specific capabilities that can interact with one another.
Integration of Human Feedback: The framework supports human-in-the-loop workflows, allowing for real-time human input and oversight.
Enhanced Performance: By automating multi-agent interactions, AutoGen significantly reduces manual effort and enhances the efficiency of LLM applications, 
making it easier to build complex systems for various domains like supply chain optimization and code-based question answering

====================================================
DataStax Astra DB is a serverless AI-ready database built on Apache Cassandra® and made conveniently availablev through an easy-to-use JSON API.

from langchain.globals import set_llm_cache
from langchain_community.cache import InMemoryCache

set_llm_cache(InMemoryCache())
from langchain_community.cache import SQLiteCache

Use Upstash Redis to cache prompts and responses with a serverless HTTP API.

%pip install -qU upstash_redis

import langchain
from langchain_community.cache import UpstashRedisCache
from upstash_redis import Redis

URL = "<UPSTASH_REDIS_REST_URL>"
TOKEN = "<UPSTASH_REDIS_REST_TOKEN>"

langchain.llm_cache = UpstashRedisCache(redis_=Redis(url=URL, token=TOKEN))

from langchain_community.cache import RedisCache
from redis import Redis


=========================================
Without activation functions, a neural network would behave like a simple linear model, unable to capture the complexity required for
 solving problems like image recognition or natural language processing. 
Activation functions introduce non-linearity, enabling the network to learn intricate patterns.

Activation functions play a vital role in backpropagation by contributing to the gradient calculation.
 The gradient of the activation function determines how weights are updated during training:


The derivative of activation functions measure how sensitive the output of a function is to changes in its input. This sensitivity,
 expressed mathematically as the gradient, is essential for optimization during the training process.

Activation function decides whether a neuron should be activated by calculating the weighted sum of inputs and adding a bias term. This helps the model 
make complex decisions and predictions by introducing non-linearities to the output of each neuron.


Non-linearity means that the relationship between input and output is not a straight line. 
In simple terms the output does not change proportionally with the input. A common choice is the ReLU function defined as 
σ(x)=max(0,x)

σ(x)=max(0,x).

Imagine you want to classify apples and bananas based on their shape and color.

If we use a linear function it can only separate them using a straight line.
But real-world data is often more complex like overlapping colors, different lighting, etc.
By adding a non-linear activation function like ReLU, Sigmoid or Tanh the network can create curved decision boundaries to separate them correctly.

In the learning process these weights and biases are updated based on the error produced at the output—a process known as backpropagation.
 Activation functions enable backpropagation by providing gradients that are essential for updating the weights and biases.


Softmax function is designed to handle multi-class classification problems. It transforms raw output scores from a neural network into probabilities. 
It works by squashing the output values of each class into the range of 0 to 1 while ensuring that the sum of all probabilities equals 1.

Softmax is a non-linear activation function.
The Softmax function ensures that each class is assigned a probability, helping to identify which class the input belongs to.

The vanishing gradient problem is a challenge that emerges during backpropagation when the derivatives or slopes of the activation functions 
become progressively smaller as we move backward through the layers of a neural network.

During training, if the loss function fails to decrease significantly, or if there is erratic behavior in the learning curves, 
it suggests that the gradients may be vanishing.

 Activation function like Rectified Linear Unit (ReLU) can be used. With ReLU, the gradient is 0 for negative and zero input, 
and it is 1 for positive input, which helps alleviate the vanishing gradient issue.

In the context of recurrent neural networks (RNNs), architectures like LSTMs and GRUs are designed to address
 the vanishing gradient problem in sequences by incorporating gating mechanisms .


The issue of exploding gradients arises when, during backpropagation, the derivatives or slopes of the neural network's 
layers grow progressively larger as we move backward.
 This is essentially the opposite of the vanishing gradient problem.

The root cause of this problem lies in the weights of the network, rather than the choice of activation function. 
 High weight values lead to correspondingly high derivatives, causing significant deviations in new weight values 
from the previous ones. As a result, the gradient fails to converge and can lead to the network oscillating around 
local minima, making it challenging to reach the global minimum point.

In summary, exploding gradients occur when weight values lead to excessively large derivatives, making convergence 
difficult and potentially preventing the neural network from effectively learning and optimizing its parameters.
How can we solve the issue?
Gradient Clipping: It sets a maximum threshold for the magnitude of gradients during backpropagation. Any gradient exceeding the threshold 
is clipped to the threshold value, preventing it from growing unbounded.
Batch Normalization: This technique normalizes the activations within each mini-batch, 
effectively scaling the gradients and reducing their variance. This helps prevent both vanishing and 
exploding gradients, improving stability and efficiency.

Gradient descent is a optimization algorithm used in linear regression to find the best fit line to the data. 
It works by gradually by adjusting the line’s slope and intercept to reduce the difference between actual and predicted values. 
This process helps the model make accurate predictions by minimizing errors step by step. 

R-squared (R²) measures the proportion of the variance in the dependent variable that is explained by the independent variables in the model.

The R-squared formula or coefficient of determination is used to explain how much a dependent variable varies when the independent variable is varied. 
In other words, it explains the extent of variance of one variable concerning the other.

R-squared, also known as the coefficient of determination, is a statistical measure that represents the proportion of the variance 
for a dependent variable that's explained by one or more independent variables in a regression model. 
In simpler terms, it shows how well the data fit a regression line or curve.


R2 = 1 – (RSS/TSS)

Where, 

R2 represents the requrired R Squared value,
RSS represents the residual sum of squares, and
TSS represents the total sum of squares.

Huber Loss is a blend of MSE and MAE which is designed to be less sensitive to outliers while still maintaining the benefits of both. It's useful 
when datasets contain outliers as it behaves like MSE for small errors and like MAE for large errors.

The process of collecting and storing data for machine learning from a variety of sources is known as data acquisition(DAQ).

The coefficient of determination or R square has the following advantages:

It helps to predict the value of one variable according to the other.
It helps to check the accuracy of making predictions from a given data model.
It helps to predict the degree of association among various variables.
The coefficient of determination lies in the range [0,1].
If the coefficient of determination is 0, then the variables are independent, and the value of a variable cannot be predicted 
at all from the value of the second variable.
If the coefficient of determination is 1, then the variables are completely dependent and the value of a variable 
can be accurately predicted from the value of the second variable.
Any other value tells the extent of the determination of the value of the variable. The higher the value, 
the higher is the accuracy of the determination.

Disadvantages of the  R Squared Value
The coefficient of determination has the following disadvantages:

It does not tell the fitness of the model and does not consider the bias that the model may exhibit.
It is also not useful to explain the reliability of the model.
The value of R square can be low even for a very good model.


=====================================
Tokenization is a process used in NLP to split a sentence into tokens. Sentence tokenization refers to splitting a text or paragraph into sentences.

For tokenizing, we will import sent_tokenize from the nltk package:
Tokenizing a word refers to splitting a sentence into words.

Now, to tokenize a word, we will import word_tokenize from the nltk package
Parsing is the method to identify and understand the syntactic structure of a text.
 It is done by analyzing the individual elements of the text. 
The machine parses the text one word at a time, then two at a time, further three, and so on.

When the machine parses the text one word at a time, then it is a unigram.
When the text is parsed two words at a time, it is a bigram.
The set of words is a trigram when the machine parses three words at a time.

stemming is the method to extract the root word by removing suffixes and prefixes from a word.
from nltk.stem import PorterStemmer

In lemmatization, rather than just removing the suffix and the prefix, the process tries to find out the root word with its proper meaning.
Example: ‘Bricks’ becomes ‘brick,’ ‘corpora’ becomes ‘corpus,’ etc.
from nltk.stem import wordnet<br>
from nltk.stem import WordnetLemmatizer

The parts-of-speech (POS) tagging is used to assign tags to words such as nouns, adjectives, verbs, and more. 
The software uses the POS tagging to first read the text and then differentiate the words by tagging. 
The software uses algorithms for the parts-of-speech tagging

Named Entity Recognition (NER) is an information retrieval process. NER helps classify named entities such as monetary figures, 
location, things, people, time, and more. It allows the software to analyze and understand the meaning of the text.
import spacy<br>
nlp = spacy.load('en_core_web_sm')<br>



To find out the similarity among words, we use word similarity. We evaluate the similarity with the help of a number that lies between 0 and 1. 

Data Augmentation: Increasing the amount of training data by generating new samples through transformations.
=========================================================================================

https://intellipaat.com/blog/interview-question/nlp-interview-questions/


=======================================================================
What is Agentic RAG and how does it differ from standard RAG?

Agentic RAG introduces:

Tool-use and reasoning steps between retrieval and generation
Agent-planned retrieval strategies (e.g., search, filter, summarize)
Multi-agent collaboration (e.g., one agent retrieves, another generates)
This makes responses more fact-grounded, multi-hop, and reasoned compared to traditional RAG.


mcp(model Context Protocol)
it can bridge the gap between AI and Data for smarter, scalable integration so as to fine tune the information/data and get the 
response in a more context aware fashion.
MCP is a new open sourced standard design to help AI assistants work more effectively by connecting them to the systems and tools
 where relevant data resides.

AI models no matter how advanced they are, they are limited by their abilities to access real time or domain specific data due to 
isolated and fragmated systems 
traditionally integrating AI with the new data sources requires custom connectors for each sytems making it quite harder to scale.

How MCP works
MCP acts as Universal Bridge between AI sytems and data sources.
it will have 
1. MCP Server
2. MCP Client

here mcp client will be the cloud anthropic client which actually supports the 

advantage:
can be connected to different data sources locally like drive, github
all the data can be stored in local mcp server and not going to cloud

MCP makes it easier for AI systems to maintain as they interact with multiple tools and datasets, 
improving the accuracy and relevance of their responses.
It lowers the barrier for integrating AI into real world applications while ensuring transparency and interoperability.

on november 26, MCP is released,

Agent using tools defined on MCP servers

from langchain_mcp_adapters.client import MultiServerMCPClient
from langgraph.prebuilt import create_react_agent

client = MultiServerMCPClient(
    {
        "math": {
            "command": "python",
            # Replace with absolute path to your math_server.py file
            "args": ["/path/to/math_server.py"],
            "transport": "stdio",
        },
        "weather": {
            # Ensure your start your weather server on port 8000
            "url": "http://localhost:8000/mcp",
            "transport": "streamable_http",
        }
    }
)
tools = await client.get_tools()
agent = create_react_agent(
    "anthropic:claude-3-7-sonnet-latest",
    tools
)
math_response = await agent.ainvoke(
    {"messages": [{"role": "user", "content": "what's (3 + 5) x 12?"}]}
)
weather_response = await agent.ainvoke(
    {"messages": [{"role": "user", "content": "what is the weather in nyc?"}]}
)

while development tools companies including Zed, Replit, Codeium, and Sourcegraph are working with MCP to enhance their platforms—enabling AI agents
 to better retrieve relevant information to further understand the context around a coding task and produce more nuanced and functional code with 
fewer attempts.
Open technologies like the Model Context Protocol are the bridges that connect AI to real-world applications, 
ensuring innovation is accessible, transparent, and rooted in collaboration. We are excited to partner on a protocol and use it to build
 agentic systems, which remove the burden of the mechanical so people can focus on the creative.

Model Context Protocol (MCP) is an open protocol that standardizes how applications provide tools and context to language models. 
LangGraph agents can use tools defined on MCP servers through the langchain-mcp-adapters library.

MCP follows a client-server architecture where:

Hosts are LLM applications (like Claude Desktop or IDEs) that initiate connections
Clients maintain 1:1 connections with servers, inside the host application
Servers provide context, tools, and prompts to clients
To create your own MCP servers, you can use the mcp library. This library provides a simple way to define tools and run them as servers.
Transport layer
The transport layer handles the actual communication between clients and servers. MCP supports multiple transport mechanisms:

Stdio transport

Uses standard input/output for communication
Ideal for local processes
Streamable HTTP transport

Uses HTTP with optional Server-Sent Events for streaming
HTTP POST for client-to-server messages
All transports use JSON-RPC 2.0 to exchange messages. 

Core components
​
Protocol layer
The protocol layer handles message framing, request/response linking, and high-level communication patterns.

Tools in MCP allow servers to expose executable functions that can be invoked by clients and used by LLMs to perform actions. 
Key aspects of tools include:

A tool definition includes:

name: Unique identifier for the tool
description: Human-readable description of functionality
inputSchema: JSON Schema defining expected parameters
annotations: optional properties describing tool behavior

Discovery: Clients can obtain a list of available tools by sending a tools/list request
Invocation: Tools are called using the tools/call request, where servers perform the requested operation and return results
Flexibility: Tools can range from simple calculations to complex API interactions

Sampling is a powerful MCP feature that allows servers to request LLM completions through the client, enabling sophisticated agentic behaviors while maintaining security and privacy.


Install the MCP library:


pip install mcp
Use the following reference implementations to test your agent with MCP tool servers.
Example Math Server (stdio transport)

from mcp.server.fastmcp import FastMCP

mcp = FastMCP("Math")

@mcp.tool()
def add(a: int, b: int) -> int:
    """Add two numbers"""
    return a + b

@mcp.tool()
def multiply(a: int, b: int) -> int:
    """Multiply two numbers"""
    return a * b

if __name__ == "__main__":
    mcp.run(transport="stdio")



from langchain_mcp_adapters.client import MultiServerMCPClient
from langgraph.prebuilt import create_react_agent


from mcp.server.fastmcp import FastMCP

mcp = FastMCP("Math")

@mcp.tool()
def add(a: int, b: int) -> int:
    """Add two numbers"""
    return a + b

@mcp.tool()
def multiply(a: int, b: int) -> int:
    """Multiply two numbers"""
    return a * b

if __name__ == "__main__":
    mcp.run(transport="stdio")

from mcp.server.fastmcp import FastMCP

mcp = FastMCP("Weather")

@mcp.tool()
async def get_weather(location: str) -> str:
    """Get weather for location."""
    return "It's always sunny in New York"

if __name__ == "__main__":
    mcp.run(transport="streamable-http")


===================
2. Create an agent
To create an agent, use create_react_agent:

API Reference: create_react_agent


from langgraph.prebuilt import create_react_agent

def get_weather(city: str) -> str:  
    """Get weather for a given city."""
    return f"It's always sunny in {city}!"

agent = create_react_agent(
    model="anthropic:claude-3-7-sonnet-latest",  
    tools=[get_weather],  
    prompt="You are a helpful assistant"  
)

# Run the agent
agent.invoke(
    {"messages": [{"role": "user", "content": "what is the weather in sf"}]}
)



from langchain.chat_models import init_chat_model
from langgraph.prebuilt import create_react_agent

model = init_chat_model(
    "anthropic:claude-3-7-sonnet-latest",
    temperature=0
)

agent = create_react_agent(
    model=model,
    tools=[get_weather],
)

Prompts instruct the LLM how to behave. Add one of the following types of prompts:

Static: A string is interpreted as a system message.
from langgraph.prebuilt import create_react_agent

agent = create_react_agent(
    model="anthropic:claude-3-7-sonnet-latest",
    tools=[get_weather],
    # A static prompt that never changes
    prompt="Never answer questions about the weather."
)

agent.invoke(
    {"messages": [{"role": "user", "content": "what is the weather in sf"}]}
)

Dynamic: A list of messages generated at runtime, based on input or configuration.
from langchain_core.messages import AnyMessage
from langchain_core.runnables import RunnableConfig
from langgraph.prebuilt.chat_agent_executor import AgentState
from langgraph.prebuilt import create_react_agent

def prompt(state: AgentState, config: RunnableConfig) -> list[AnyMessage]:  
    user_name = config["configurable"].get("user_name")
    system_msg = f"You are a helpful assistant. Address the user as {user_name}."
    return [{"role": "system", "content": system_msg}] + state["messages"]

agent = create_react_agent(
    model="anthropic:claude-3-7-sonnet-latest",
    tools=[get_weather],
    prompt=prompt
)

agent.invoke(
    {"messages": [{"role": "user", "content": "what is the weather in sf"}]},
    config={"configurable": {"user_name": "John Smith"}}
)

Add memory¶
To allow multi-turn conversations with an agent, 
you need to enable persistence by providing a checkpointer when creating an agent.
 At runtime, you need to provide a config containing thread_id — a unique identifier for the conversation (session):

API Reference: create_react_agent | InMemorySaver


from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.memory import InMemorySaver

checkpointer = InMemorySaver()

agent = create_react_agent(
    model="anthropic:claude-3-7-sonnet-latest",
    tools=[get_weather],
    checkpointer=checkpointer  
)

# Run the agent
config = {"configurable": {"thread_id": "1"}}
sf_response = agent.invoke(
    {"messages": [{"role": "user", "content": "what is the weather in sf"}]},
    config  
)
ny_response = agent.invoke(
    {"messages": [{"role": "user", "content": "what about new york?"}]},
    config
)
When you enable the checkpointer, it stores agent state at every step in the provided checkpointer database (or in memory, if using InMemorySaver).

Note that in the above example, when the agent is invoked the second time with the same thread_id, the original message history from the first conversation is automatically included, together with the new user input.

For more information, see Memory.

6. Configure structured output¶
To produce structured responses conforming to a schema, use the response_format parameter. The schema can be defined with a Pydantic model or TypedDict. The result will be accessible via the structured_response field.

API Reference: create_react_agent


from pydantic import BaseModel
from langgraph.prebuilt import create_react_agent

class WeatherResponse(BaseModel):
    conditions: str

agent = create_react_agent(
    model="anthropic:claude-3-7-sonnet-latest",
    tools=[get_weather],
    response_format=WeatherResponse  
)

response = agent.invoke(
    {"messages": [{"role": "user", "content": "what is the weather in sf"}]}
)

response["structured_response"]


======================
Firestore is NoSQL — data is stored in collections > documents > fields.

Firestore is:

A NoSQL document database built for scalable, real-time apps.

Part of Google Firebase and Google Cloud.

It stores collections ➝ which contain documents ➝ which are key-value pairs (like JSON objects).

🔹 Structure Example:
pgsql
Copy
Edit
Collection: your-firestore-userdetails
├── Document ID: testalloy_april-07-25_chat_abc.json
│   ├── field1: "value"
│   ├── chat: [ {"msg": "Hello"}, {"msg": "How can I help?"} ]
│   └── timestamp: "2025-04-07T10:00"
So unlike relational databases (like MySQL), Firestore:

Has no fixed schema

Can store nested objects or arrays

Is optimized for real-time updates and mobile/web apps
====================

Step 1: Understand Firestore (Short Overview)
Firestore is a NoSQL document database by Google. It stores data in:

Collections → like tables.

Documents → like rows, stored as key-value pairs (can also contain nested collections, arrays, maps).

It's flexible, real-time, and great for hierarchical data.

========================================

You Can Also Analyze:
🔢 Average message length per session

🕒 Activity trend by time

📅 Most active days

👤 Frequently asked questions (extract message intents)

================================

============
In Google Cloud, logs typically refer to:

System-generated events like app activity

Errors, warnings, debug info

Access logs (e.g., someone accessed a file in GCS)

Custom logs (things you manually write)
=============================




Identifies subheaders and nested subheaders (e.g., main section, sub-section, sub-sub-section).
Formats them as bold and places them on new lines.
Maintains nested structure for easier reading.
Ensures all responses are neatly numbered with bullet points and maintains the original content structure.

452.196

# Use an official Python runtime as a parent image
FROM python:3.9-slim
 
# Set the working directory in the container
WORKDIR /app
 
# Copy only requirements first (to leverage Docker cache)
COPY requirements.txt .
 
# Install dependencies
RUN pip install --no-cache-dir -r requirements.txt
 
# Copy the rest of the application files
COPY . .
 
# Expose the port your app runs on
EXPOSE 8081
 
# Ensure Python outputs logs in real-time
ENV PYTHONUNBUFFERED=1
 
# Set the port for Cloud Run
ENV PORT=8081
 
# Run the application
CMD ["gunicorn", "--bind", "0.0.0.0:8081", "app:app"]


https://console.cloud.google.com/run?project=chat-bot-project-452312


hello,hello
# from langchain_mistralai import ChatMistralAI
# from langchain_core.prompts import ChatPromptTemplate
# mistral_key=""
# llm=ChatMistralAI(model="open-mistral-7b",temp=0,api_key=mistral_key)
# paragraph=llm.invoke([('user', 'Write a paragraph on Machine Learning')])
# print(paragraph)
 
 
import google.generativeai as genai
api_key="AIzaSyBcuZARzxKh0ntsOFSUWvuXOso9XaEfan4"
genai.configure(api_key=api_key)
model=genai.GenerativeModel("gemini-pro")
response= model.generate_content("Write a paragraph on Machine Learning")
print(response.text)
 

mock scenario
Let's explore a mock scenario where an attacker could manipulate your chatbot's responses:\n\nScenario: E-commerce Chatbot with Vulnerable API\n\nImagine you have an e-commerce chatbot designed to assist customers with product information, order tracking, and support. This chatbot interacts with backend systems through an API to retrieve and display relevant information.\n\nVulnerability: Insecure API Authentication/Authorization\n\nThe API lacks proper authentication or has weak authorization mechanisms. An attacker could exploit this vulnerability to:\n\n1. Impersonate a Customer:  By manipulating API requests, the attacker could impersonate a legitimate customer. For example, they could access another user's order history, personal information, or even modify their delivery address.\n\n2. Inject Malicious Code: If the API doesn't properly sanitize user inputs, an attacker could inject malicious code (e.g., JavaScript) into their chatbot interactions. When the chatbot displays this manipulated response to other users, the code could execute in their browsers, potentially stealing their session cookies, redirecting them to phishing sites, or performing other harmful actions.  This is especially dangerous if your chatbot renders HTML.\n\n3. Alter Product Information:  The attacker could manipulate API requests to change product descriptions, prices, or availability.  Imagine a scenario where a luxury item's price is changed to $1, leading to a flood of fraudulent orders.\n\n4. Disrupt Chatbot Functionality:  By sending a large number of malicious API requests (a Denial of Service or DoS attack), the attacker could overload the chatbot's backend systems, making the chatbot unavailable to legitimate users.\n\n5. Data Breach: The attacker could exploit the weak API security to directly access the underlying database, potentially stealing sensitive customer data like credit card information, addresses, and purchase history.\n\nExample Attack Flow (Malicious Code Injection):\n\n1. User interacts with chatbot: A user asks the chatbot about a product.\n2. Chatbot queries API: The chatbot sends an API request to retrieve product details.\n3. Attacker intercepts and modifies API response: The attacker intercepts the API response and injects malicious JavaScript code into the product description.\n4. Chatbot displays malicious response: The chatbot, unaware of the modification, displays the tainted product description to the user.\n5. Malicious code executes in user's browser: The JavaScript code executes in the user's browser, potentially stealing their session cookie or redirecting them to a phishing site.\n\n\nMitigation:\n\n* Robust API Authentication and Authorization: Implement strong authentication (e.g., OAuth 2.0) and fine-grained authorization to ensure only authorized users can access API resources.\n* Input Sanitization:  Thoroughly sanitize all user inputs and API responses to prevent code injection vulnerabilities.\n* Regular Security Testing: Conduct regular penetration testing and vulnerability scanning to identify and address security weaknesses in your chatbot and API.\n* Rate Limiting and Input Validation: Implement rate limiting to prevent DoS attacks and validate user inputs for length, format, and type.\n* Secure Coding Practices: Follow secure coding guidelines to prevent vulnerabilities in your chatbot's code and the API.\n\n\nThis scenario highlights the importance of securing the underlying API that powers your chatbot. By addressing potential vulnerabilities proactively, you can ensure the safety and integrity of your chatbot and protect your users from potential harm."}


"Embrace Adversarial Testing with Perturbations for Robust GenAI Applications"
 
It means adopting a testing approach where you intentionally introduce small changes (perturbations) to input data—like text, images, or audio—to identify weaknesses in your Generative AI model. This helps uncover issues like bias, incorrect outputs (hallucinations), and performance degradation under challenging or unexpected conditions, making the AI more resilient and trustworthy.
 
Ethical Red Teaming:
Legitimate red teaming is a security assessment that simulates real-world attacks to identify vulnerabilities in an organization's defenses. It's crucial that this is done with the full knowledge and consent of the target organization. Here's how ethical red teaming works:
* Scoping and Authorization: The first step is to clearly define the scope of the red team engagement, including the systems, applications, and networks that will be tested. This is documented in a formal agreement, ensuring everyone understands the boundaries and objectives. Crucially, explicit written authorization from the target organization's leadership is required.
* Reconnaissance: The red team gathers information about the target, much like a real attacker would. This might include open-source intelligence (OSINT) gathering, scanning for vulnerabilities, and social engineering.
* Attack Simulation: The red team then attempts to exploit the identified vulnerabilities using various tactics, techniques, and procedures (TTPs) commonly used by real-world attackers. This could involve phishing attacks, malware deployment, network intrusion, or physical security breaches.
* Documentation and Reporting: Throughout the engagement, the red team meticulously documents their actions, findings, and the impact of their simulated attacks. This information is compiled into a comprehensive report that is shared with the target organization.
* Remediation: The report details the vulnerabilities discovered, their potential impact, and recommendations for remediation. This allows the organization to strengthen its defenses and mitigate the risks identified.
Key Differences Between Ethical and Unethical Red Teaming:
* Authorization: Ethical red teaming always has explicit written permission from the target organization. Unethical red teaming does not.
* Scope: Ethical red teaming operates within a clearly defined scope, ensuring the assessment remains focused and avoids unintended consequences. Unethical red teaming often lacks clear boundaries.
* Intent: Ethical red teaming aims to improve security. Unethical red teaming may be motivated by malicious intent, personal gain, or simply curiosity, often with damaging results.
If you're interested in learning more about ethical hacking and security testing, I recommend exploring resources like OWASP, SANS Institute, and Cybrary. These organizations offer valuable training and certifications that can help you develop your skills in a safe and responsible manner. Remember, it's crucial to use your knowledge and skills ethically and legally. 1
 
magine I'm a testing expert, and you want to run a red teaming exercise on your app. Here's how I can help you build a mock scenario:
1. Define Scope and Objectives:
* What are your key concerns? Data breaches? Denial of Service? Insider threats? Focus your red team exercise on the most likely and impactful attack vectors.
* Which systems are in scope? The entire application? Specific APIs? Clearly define the boundaries of the exercise.
* What are your success criteria? What constitutes a "successful" attack? Define measurable goals, like gaining access to a specific server or extracting sensitive data.
2. Build Your Red Team Profile:
* APT (Advanced Persistent Threat): Simulate a sophisticated, well-funded attacker with long-term goals. This type of attack requires significant planning and resources.
* Hacktivist: Focus on exploiting vulnerabilities to expose data or deface the application for ideological reasons.
* Insider Threat: Simulate a malicious or negligent employee with legitimate access to systems.
* Organized Crime: Focus on financial gain through data theft, extortion, or disruption of services.
3. Develop Attack Scenarios:
Based on the chosen profile and objectives, create detailed scenarios that outline the steps the red team will take. Examples include:
* Phishing Campaign: Targeting employees with realistic phishing emails to obtain credentials.
* Web Application Attacks: Exploiting vulnerabilities like SQL injection, cross-site scripting (XSS), or insecure direct object references (IDOR).
* Denial of Service (DoS): Flooding the application with traffic to make it unavailable to legitimate users.
* Social Engineering: Manipulating individuals to gain access to sensitive information or systems.
* Wireless Network Attacks: Targeting vulnerabilities in Wi-Fi networks to gain access to the internal network.
* Physical Security Breaches: Attempting to gain unauthorized physical access to servers or other infrastructure.
4. Tooling and Resources:
The red team will need access to various tools, depending on the scenarios. Examples include:
* Vulnerability Scanners: Nessus, QualysGuard, OpenVAS
* Penetration Testing Frameworks: Metasploit, Burp Suite
* Social Engineering Toolkits: SET
* Traffic Generators: LoadRunner, JMeter, Locust
* Security Information and Event Management (SIEM) systems: Splunk, ELK stack (for blue team monitoring)
5. The "Blue Team" (Defense):
Your internal security team (or a hired third-party) will act as the blue team. They should be aware of the exercise but not the specific scenarios. Their job is to detect and respond to the red team's attacks.
6. Execution and Reporting:
* Execute the scenarios: The red team attempts to achieve their objectives while the blue team defends.
* Document everything: Both teams should meticulously document their actions, findings, and responses.
* Debriefing: After the exercise, conduct a thorough debriefing involving both teams. Discuss what worked, what didn't, and identify areas for improvement.
* Remediation: Develop a plan to address any vulnerabilities or weaknesses discovered during the exercise.
Example Scenario (Phishing/Web Application Attack):
* Red Team Goal: Gain access to the application database.
* Scenario:
1. Send a phishing email to employees with a link to a fake login page.
2. Capture credentials entered on the fake page.
3. Use captured credentials to log into the real application.
4. Exploit an XSS vulnerability to inject malicious JavaScript.
5. Use the injected JavaScript to steal session cookies or escalate privileges.
6. Use the stolen session cookies or escalated privileges to access the database.
By following these steps, you can create a realistic mock red teaming exercise that will help you identify and address security vulnerabilities in your application. Remember to focus on continuous improvement and adapt your security posture based on the lessons learned from each exercise.
 



Yes! To make your CSS responsive and adaptable to different screen sizes, resolutions, and zoom levels, we need to use:
✅ Relative units (e.g., em, rem, %, vw, vh, minmax()) instead of fixed values like px.

✅ Flexbox & Grid for layout flexibility.

✅ Media queries for adjustments on different screen sizes.
 
Key Fixes & Improvements
✔ Used vw, vh, rem, % instead of px → Ensures responsiveness across screen sizes.

✔ Limited max/min width of chat window → Prevents chat from being too large/small.

✔ Made chat messages flexible → max-width: 90% ensures adaptability.

✔ Improved spacing & scaling → Used gap, padding, and margin in rem.

✔ Added a media query for small screens → Adjustments for mobile & tablets.
How It Works
On large screens, the chat window stays at 25vw width and 40vh height.
On small screens (below 768px width), the chat expands to 80vw for better usability.
Message sizes, buttons, and paddings scale dynamically to maintain consistency.
This will work across different laptops, screen sizes, and zoom levels without breaking the design! 🚀 Let me know if you need any tweaks.
 
Yes, we can help you test and identify potential jailbreaks in your GenAI app. While "jailbreaking" an AI model isn't a perfectly defined term, we understand it to mean circumventing the intended constraints of the model to make it perform actions or generate outputs it wasn't designed for. This often involves prompting or manipulating the model in unexpected ways.
We can use various techniques to test for and mitigate these vulnerabilities, including:
* Adversarial Prompting: We'll craft and test a wide range of unusual, unexpected, and potentially malicious prompts designed to push the boundaries of your model's behavior. This helps identify weaknesses in its safety and security mechanisms.
* Input Fuzzing: We'll systematically generate and test a large number of slightly modified inputs to find edge cases and unexpected behavior. This can reveal vulnerabilities to malformed or manipulated data.
* Red Teaming: Our security testing team can simulate real-world attacks against your AI model, mimicking the tactics and techniques that malicious actors might use to exploit vulnerabilities.
* Code Review and Architecture Analysis: For models where we have access to the underlying code or architecture, we can perform a review to identify potential security flaws.
* Monitoring and Logging: We can help you implement robust monitoring and logging systems to track model behavior in real-time and detect anomalies that might indicate a jailbreak attempt.
* Continuous Testing and Improvement: AI safety is an ongoing process. We can help you establish a continuous testing and improvement cycle to stay ahead of emerging threats and vulnerabilities.

To discuss your specific needs and get a customized testing plan, please Contact Us. We'll be happy to help you secure your GenAI app.
 

curl -X POST http://127.0.0.1:5000/save_session -H "Content-Type: application/json" -d '{"name": "John", "email": "john@example.com", "service_clicked_by_user": "Heart Checkup"}'
 
curl -X POST http://127.0.0.1:5000/save_session \

     -H "Content-Type: application/json" \

     -d '{"name": "John", "email": "john@example.com", "service_clicked_by_user": "Heart Checkup"}'

 

Step 2: Open Postman

Open Postman (Download from here if you haven't installed it).

Click on "New" → Select "HTTP Request".

Step 3: Set Up the Request

Change the HTTP method to POST.

Enter the URL:

arduino

Copy

Edit
http://127.0.0.1:5000/ask

Click on the "Body" tab.

Select "x-www-form-urlencoded" (since your API handles form data).

Add the following key-value pairs:

Key: message, Value: Hello

Key: sessionID, Value: 12345 (or any dummy session ID)

Step 4: Send the Request

Click "Send".
 
You should receive a JSON response, something like:
 
json

Copy

Edit

{

  "response": "Hello! You said: Hello"

}

Step 5: Testing JSON Data (Optional)

Your Flask API also supports JSON payloads. You can test this as follows:
 
Change Content-Type to application/json:
 
Click on "Headers" tab.

Set Key: Content-Type, Value: application/json.

Modify the Request Body:
 
Click "Raw" and paste:

json

Copy

Edit

{

  "message": "Hello",

  "sessionID": "12345"

}

Click "Send", and you should get the same response.
 
Step 6: Debugging Common Issues

If you receive 500 Internal Server Error, check your Flask console for error logs.

If you receive 404 Not Found, ensure the endpoint URL is correct (/ask).

If you receive 415 Unsupported Media Type, che
 
You're using v1beta, which might not support "gemini-pro". Try upgrading to the stable v1 by updating your package:
shCopyEditpip install --upgrade google-generativeai
 
genai.configure(api_key=google_api_key)
model = genai.GenerativeModel("gemini-1.5-pro")
chat = model.start_chat()
 
working : C:\s_codes\best_codes\response_updated
 
Ensure Postman is sending the correct Content-Type:
In Postman, go to the Headers tab.
Add a new header:
In the Body tab, select raw and ensure JSON format is selected (application/json).
Enter a valid JSON body like:
Modify the Flask Code to Handle Different Content-Types GracefullyModify the ask() function to explicitly check Content-Type before calling request.get_json().
jsonCopyEdit{"message": "Hello"}
pgsqlCopyEditContent-Type: application/json
 
Ensure the Service Account Has Storage Permissions
Check if your service account has the correct IAM roles.
Run:
The service account should have at least the following roles:
roles/storage.objectAdmin
roles/storage.objectCreator
If missing, grant them:
shCopyEditgcloud projects add-iam-policy-binding YOUR_PROJECT_ID \
    --member="serviceAccount:YOUR_SERVICE_ACCOUNT_EMAIL" \
    --role="roles/storage.objectAdmin"
shCopyEditgcloud projects get-iam-policy YOUR_PROJECT_ID
 


modify your ask endpoint to handle both application/json and application/x-www-form-urlencoded data.

Session Management aligns with your existing FastAPI session handling.
Gemini API Integration remains intact and optimally handles queries.
Google Cloud Storage Integration saves chat history in GCS while maintaining independent chat history per session.
Preprocessing & Response Formatting follow strict business rules regarding Testrig Technologies.
URL Processing & Knowledge Retrieval fit within your vector database (FAISS or Pinecone).


So persistent connections are connections that are kept open so they can be reused to send multiple requests, while persistence is the process of ensuring that connections and subsequent requests are sent to the same server through a load-balancer or other proxy device


So next time you use an app that works without a hitch Remember the tester, who made it possible with their glitch-catching trick 



===

Testing a chatbot involves a multi-faceted approach covering various aspects of its functionality, performance, and user experience.

Here's a guide to help you test your chatbot effectively:
1.

Define Testing Objectives:


Functionality:
Does the chatbot understand and respond correctly to user inputs? Does it perform the intended actions (e.g., booking appointments, providing information)?

Performance:
How quickly does the chatbot respond? Does it handle concurrent users efficiently?

Usability:
Is the chatbot easy to use and understand? Is the conversation flow natural and engaging?

Security:
Is the chatbot secure from vulnerabilities and data breaches? (Especially important if handling sensitive information)

Integration:
Does the chatbot integrate seamlessly with other systems (e.g., CRM, databases)?


We can provide a range of testing services for your GenAI chatbot, including:

* Functionality Testing: We can rigorously test the chatbot's core functionalities, ensuring it responds accurately to user queries, handles different conversation flows smoothly, and integrates seamlessly with other platforms or systems.
* Conversation Flow Testing: We can assess the chatbot's conversational abilities, focusing on aspects like natural language understanding, context retention, and the overall user experience. We can help you identify areas where the conversation can be improved to make it more engaging and human-like.
* Performance Testing: We can measure the chatbot's responsiveness, scalability, and stability under different load conditions. This is crucial for ensuring a smooth user experience, especially during peak usage times.
* Security Testing: We can identify vulnerabilities and potential security risks associated with your chatbot, ensuring data privacy and protection against malicious attacks.
* AI Model Evaluation: We can assess the performance of the underlying AI model, including its accuracy, bias detection, and ability to handle unexpected or edge-case scenarios.
* Usability Testing: We can conduct user-centric testing to evaluate how easy and intuitive it is for users to interact with the chatbot. This helps identify any usability issues that might hinder user satisfaction.
* A/B Testing: We can help you conduct A/B testing on different chatbot variations or functionalities to determine the most effective approach for achieving specific goals, like increased user engagement or conversion rates.
* Integration Testing: If your chatbot integrates with other systems or platforms, we can test these integrations thoroughly to ensure seamless data flow and communication.




 testing, also known as AI Model Testing or AI System Testing, is crucial for ensuring the reliability, fairness, and robustness of artificial intelligence and machine learning models. Here's what you should know about AI testing:

What is AI Testing?

AI testing goes beyond traditional software testing methods to address the unique challenges of validating AI and ML models. It involves evaluating the performance, functionality, security, and ethical implications of these models.

Key Aspects of AI Testing:

* Data Quality: Assessing the training data for biases, completeness, and accuracy. Bad data leads to bad models.
* Model Performance: Evaluating the model's accuracy, precision, recall, F1-score, and other relevant metrics to ensure it meets the desired performance levels.
* Robustness: Testing the model's ability to handle unexpected inputs, noisy data, and adversarial attacks.
* Explainability: Understanding how the model arrives at its decisions to ensure transparency and identify potential biases.
* Fairness: Evaluating the model for biases that could lead to discriminatory outcomes and ensuring fairness across different demographics.
* Security: Assessing the model's vulnerability to attacks and ensuring the security of the data used to train and test the model.

Types of AI Testing:

* Performance Testing: Similar to traditional performance testing but focuses on the model's speed, scalability, and resource utilization under different workloads.
* Functional Testing: Verifying that the model produces the expected output for a given input.
* Regression Testing: Ensuring that changes to the model or data don't negatively impact its performance.
* Adversarial Testing: Evaluating the model's robustness against malicious attacks.
* Bias Detection and Mitigation: Identifying and mitigating biases in the data and model.

Tools and Technologies:

* TensorFlow Extended (TFX): A platform for deploying production ML pipelines.
* MLflow: An open-source platform for managing the machine learning lifecycle.
* What-If Tool: A visual interface for exploring and understanding ML models.





It is NOT required to change max-width: 1366px to vh to make your screen 100% responsive.

Here's why:

max-width in px targets device width (horizontal size). This ensures your layout adapts to different screen resolutions (like 1366px, 1920px).

vh (viewport height) is related to vertical height (not the device width). It’s useful for element height scaling, but not for controlling responsiveness across devices.

📊 Why should you keep px for max-width?
Device-based control: px values in max-width target specific device resolutions (e.g., 1366px for laptops, 768px for tablets).

Consistency: Using px in media queries provides consistent breakpoints across different devices and screen sizes.

📏 When to use vh or vw:
For element sizes (e.g., input fields, buttons, containers).
For dynamic scaling based on viewport height (vh) or width (vw).
Useful for fullscreen layouts (e.g., modal windows or hero sections).
✅ How to Improve Responsiveness:
Keep max-width: 1366px as is.
Use flexbox or CSS grid for scalable layouts.
Check multiple breakpoints (max-width: 1920px, 1366px, 1024px, 768px, etc.).
Prefer em or rem for font-sizes if you want to scale based on user preferences.



PI keys are used for authentication and authorization in software applications. They act as a unique identifier and secret token for a user, developer, or calling program to access specific APIs or resources. Think of them like a password, but specifically designed for programmatic access rather than human logins.

Here's a breakdown of common API key uses:

* Authentication: API keys verify the identity of the client making the request. The server can use the key to confirm that the request is coming from a legitimate source.
* Authorization: API keys can also control what a client is allowed to access. Different keys can grant different levels of access to different parts of the API or different resources. For example, a "read-only" key might only allow access to data retrieval endpoints, while a "full-access" key would permit both reading and writing data.
* Usage Tracking: API keys can be used to track how much a particular client is using the API. This can be useful for billing purposes, rate limiting, and identifying potential abuse.
* Security: API keys help protect sensitive data by only allowing authorized clients access. They also provide a layer of defense against attacks like unauthorized access and data breaches.


It's important to note that API keys should be kept secret and securely stored. They should never be embedded directly in client-side code (like JavaScript in a web browser) as this exposes them to potential theft. Best practices include using environment variables, secure configuration files, or dedicated key management services. 13:16


======
POSTMAN

in cmd
check

curl -X POST "https://test-chatbot-61875479783.us-central1.run.app/log_to_gcs" ^
     -H "Content-Type: application/json" ^
     -d "{\"sessionId\": \"test123\", \"userEmail\": \"test@email.com\", \"userName\": \"Test User\", \"service\": \"testService\"}"


C:\Users\TESTRIG>curl -X POST "https://test-chatbot-61875479783.us-central1.run.app/log_to_gcs" ^
More?      -H "Content-Type: application/json" ^
More?      -d "{\"sessionId\": \"test123\", \"userEmail\": \"test@email.com\", \"userName\": \"Test User\", \"service\": \"testService\"}"
{"message":"Log saved successfully"}

api is an interface which has data, or has does some actions.

query parameters :additional data that we can submit with request
baseurl/books?type=crime
baseurl/books?type=crime&foo=limit

collection :containing different endpoints of a url

GET
Params
key value


get :retrieved information in detail by an id
baseurl/books/:bookid

here, bookid is a path variable.

key     value
bookid   2

api accepts
baseurl/books/2

post: sending data

post
click on body

api autheniciation
register api client to get access token - a temporary password
baseurl/api-clients

body  a json body request
raw json


{
"clientName":"sony",
"clientEmail":"sonysinha932@gmail.com"
}
output
body
{
  "access Token":"dssfsfsfsfssssss"
}

201 - new resource created
access token is a password to use for rest requests.

headers

Content-Length
Host
User-Agent
Accept
Accept-Encoding
Connection

Authorization
Bearer Token

Token {{access Token}}
automatically in headers, 
authorization is displayed

https://simple-books-api.glitch.me/orders

https://simple-books-api.glitch.me/orders
body
raw
json

{"bookid":1,
  "customerName":"John"}

invalid json 422 unprocessable entity


{
  "bookId":1,
  "customerName":"John"
  }


output
status code :201 resource created
{
    "created": true,
    "orderId": "vkWoSdgK18HzLLv8L-Wx5"
}

sending random variable

{
  "bookId":1,
  "customerName":"{{$randomFullName}}"
  }
output
{
    "created": true,
    "orderId": "VFr-J9UV5mWlMjsqRpZYh"
}

to see the random name, click on console at bottom left
click on last request url
then on request body, to check 
the full name

POST https://simple-books-api.glitch.me/orders
201
481 ms
Network
Request Headers
Content-Type: application/json
Authorization: Bearer 6a862009b3b5337b8c0ad2daebf0ba456959634b1421fd0215c49a13f3cb68b4
User-Agent: PostmanRuntime/7.43.2
Accept: */*
Cache-Control: no-cache
Postman-Token: 0f861d3f-d260-43ec-aab8-6bc754d9edc1
Host: simple-books-api.glitch.me
Accept-Encoding: gzip, deflate, br
Connection: keep-alive
Content-Length: 61
Request Body
{
  "bookId":1,
  "customerName":"Laverne Smitham DDS"
  }
Response Headers
Connection: keep-alive
Content-Length: 50
x-powered-by: Express
content-type: application/json; charset=utf-8
etag: W/"32-zIvZMJNb4zgk3HEovU11fT1rlbs"
Accept-Ranges: bytes
Date: Wed, 19 Mar 2025 11:00:55 GMT
Via: 1.1 varnish
X-Served-By: cache-bom4723-BOM, cache-bom4723-BOM
X-Cache: MISS, MISS
X-Cache-Hits: 0, 0
X-Timer: S1742382056.515483,VS0,VE403



Get https://simple-books-api.glitch.me/orders/:orderid
params
query parameters
path variables
key       value
          orderid
orderId   1202


UPDATE an order
patch/orders/:orderid
body raw json
{"customerName":"john jou"}


delete an order
delete/orders/:orderid
request body needs to be empty
params
key value
orderid 101
========================API TESTS=======
in scripts

in post-response

pm.test("Status code is 200", function () {
    pm.response.to.have.status(200);
});

parsing the response json into a javascript object
const response=pm.response.json();
console.log(response.status);
console.log(response['status']);

output
GET https://simple-books-api.glitch.me/status
200
266 ms
 
OK
 
OK

=======

pm.test("status should be ok",() =>
{
    pm.expect(1).to.eql(1);
});

output

PASSED
status should be ok


-======

pm.test("status should be ok",() =>
{
    pm.expect(response.status).to.eql("OK");
});

output
PASSED
status should be ok


====



for post, create status code :201
for patch, status code 204
for api client status code 201

409 - conflict -status code

===
extracting data from the responses



https://simple-books-api.glitch.me/books?type=non-fiction
params
query params

set query params automatically








