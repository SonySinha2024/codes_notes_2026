
Anthropic

MCP

MCP Clients and Servers: Servers host resources, tools, clients use servers.
Tools: Tools are implemented on a server and extend an LLM's capabilities.
Resources : Servers can deliver any kind of document or information to clients
Prompts : servers can provide to client with pre optimized prompts.


our server
MCP client ------ MCP Server (tools, prompts, resources)

Model Context Protocol (MCP) is a communication layer that provides Claude with context and tools without requiring you to 
write a bunch of tedious integration code. 
Think of it as a way to shift the burden of tool definitions and execution away from your server to specialized MCP servers.

showing the basic architecture: an MCP Client (your server) connecting to MCP Servers that contain tools, prompts, and resources. 
Each MCP Server acts as an interface to some outside service.

example : A github Chat interface/ github chatbot

chat interface, using a llm with tools that can access a user's Github account and answer queries like
What open pull requests are there across all my repositories?"

why MCP? 
GitHub has massive functionality - repositories, pull requests, issues, projects, and tons more. Without MCP, you'd need to create
an incredible number of tool schemas and functions to handle all of GitHub's features.

To handle all of Github's functionality, we would have to create an incredible number of tool schemas and functions.
This is all code that developers have to write, test and maintain.

How MCP Works
MCP shifts this burden by moving tool definitions and execution from your server to dedicated MCP servers. 
Instead of you authoring all those GitHub tools, an MCP Server for GitHub handles it.

The MCP Server wraps up tons of functionality around GitHub and exposes it as a standardized set of tools. 
Your application connects to this MCP server instead of implementing everything from scratch.

MCP Servers Explained
MCP Servers provide access to data or functionality implemented by outside services. 
They act as specialized interfaces that expose tools, prompts, and resources in a standardized way.


The MCP Server wraps up tons of functionality around GitHub and exposes it as a standardized set of tools. 
Your application connects to this MCP server instead of implementing everything from scratch.

MCP Servers Explained
MCP Servers provide access to data or functionality implemented by outside services. 
They act as specialized interfaces that expose tools, prompts, and resources in a standardized way.

In our GitHub example, the MCP Server for GitHub contains tools like get_repos() and connects 
directly to GitHub's API. Your server communicates with the MCP server, which handles all the GitHub-specific implementation details.

Common Questions
Who authors MCP Servers?
Anyone can create an MCP server implementation. Often, service providers themselves will make their own official MCP implementations. 
For example, AWS might release an official MCP server with tools for their various services.

How is this different from calling APIs directly?
MCP servers provide tool schemas and functions already defined for you. If you want to call an API directly, you'll be authoring 
those tool definitions on your own. MCP saves you that implementation work.

Isn't MCP just the same as tool use?
This is a common misconception. MCP servers and tool use are complementary but different concepts. 
MCP servers provide tool schemas and functions already defined for you, 
while tool use is about how Claude actually calls those tools. 
The key difference is who does the work - with MCP, someone else has already implemented the tools for you.

The benefit is clear: instead of maintaining a complex set of integrations yourself, 
you can leverage MCP servers that handle the heavy lifting of connecting to external services.


----
MCP Client

The MCP client serves as the communication bridge between your server and MCP servers. 
It's your access point to all the tools that an MCP server provides, 
handling the message exchange and protocol details so your application doesn't have to.

Transport Agnostic Communication
One of MCP's key strengths is being transport agnostic - a fancy way of saying the client and server can communicate 
over different protocols depending on your setup.

The most common setup runs both the MCP client and server on the same machine, 
communicating through standard input/output. But you can also connect them over:
HTTP
WebSockets
Various other network protocols

MCP Message Types
Once connected, the client and server exchange specific message types defined in the MCP specification. 

The main ones you'll work with are:

ListToolsRequest/ListToolsResult: The client asks the server "what tools do you provide?" and gets back a list of available tools.

CallToolRequest/CallToolResult: The client asks the server to run a specific tool with given arguments, then receives the results.

How It All Works Together

Here's a complete example showing how a user query flows through the entire system - from your server, through the MCP client, to external services like GitHub, and back to Claude.

Let's say a user asks "What repositories do I have?" Here's the step-by-step flow:

User Query: The user submits their question to your server
Tool Discovery: Your server needs to know what tools are available to send to Claude
List Tools Exchange: Your server asks the MCP client for available tools
MCP Communication: The MCP client sends a ListToolsRequest to the MCP server and receives a ListToolsResult
Claude Request: Your server sends the user's query plus the available tools to Claude
Tool Use Decision: Claude decides it needs to call a tool to answer the question
Tool Execution Request: Your server asks the MCP client to run the tool Claude specified
External API Call: The MCP client sends a CallToolRequest to the MCP server, which makes the actual GitHub API call
Results Flow Back: GitHub responds with repository data, which flows back through the MCP server as a CallToolResult
Tool Result to Claude: Your server sends the tool results back to Claude
Final Response: Claude formulates a final answer using the repository data
User Gets Answer: Your server delivers Claude's response back to the user


Yes, this flow involves many steps, but each component has a clear responsibility. The MCP client abstracts away the complexity of server 
communication, letting you focus on your application logic while still getting access to powerful external tools and data sources.


Implement A CLI based bot

CLI bot that allows users to chat with a set of documnets.
Claude should be able to read documents
Claude should be able to edit a documents
users can mention a documnet by writing out "@doc_name"
the doc's contents will automatically be included as context.

our server has our mcp client which will connect with our MCP Server with tool to read a doc and tool to update a doc, 
and document can be pdf, xlsx, txt, md, 

Note : on a normal project, we will implement either an MCP CLient or an MCP server, 

this project will implement both , for understanding MCP Client and MCP server.

python MCP SDK makes it very easy to declare tools.

@mcp.tool(
   name ="add_ints"
   description =" Add two integers")

def tool_fn(
  a =Field(description ="First mumber to add")
  b =Field(description ="second number to add")
  ) -> int:
       return a+b

from pydantic import Field
from mcp.server.fastmcp import FastMCP

mcp = FastMCP("DocumentMCP", log_level="ERROR")


docs = {
    "deposition.md": "This deposition covers the testimony of Angela Smith, P.E.",
    "report.pdf": "The report details the state of a 20m condenser tower.",
    "financials.docx": "These financials outline the project's budget and expenditures.",
    "outlook.pdf": "This document presents the projected future performance of the system.",
    "plan.md": "The plan outlines the steps for the project's implementation.",
    "spec.txt": "These specifications define the technical requirements for the equipment.",
}

# TODO: Write a tool to read a doc

@mcp.tool(
    name="read_doc_contents",
    description= "Read the contents of a document and return it as a String."
)
def read_documnet(
    doc_id: str=Field(description="Id of the document to read")
):
    if doc_id  not in docs:
        raise ValueError(f"Doc with id {doc_id} not found")
    return docs(doc_id)

# TODO: Write a tool to edit a doc

@mcp.tool(
    name="edit document",
    description ="edit a document by replacing a string in the documents with another string."
)
def edit_document(
    doc_id:str =Field(description ="Id of the document to be edited"),
    old_str:str =Field(description ="text to replace"),
    new_str:str =Field(description ="text to insert")
):
     if doc_id  not in docs:
        raise ValueError(f"Doc with id {doc_id} not found")
     docs{doc_id} =docs[doc_id].replace(old_str,new_str)
    
# TODO: Write a resource to return all doc id's
# TODO: Write a resource to return the contents of a particular doc
# TODO: Write a prompt to rewrite a doc in markdown format
# TODO: Write a prompt to summarize a doc


if __name__ == "__main__":
    mcp.run(transport="stdio")

Building an MCP server becomes much simpler when you use the official Python SDK. 
Instead of writing complex JSON schemas by hand, you can define tools with decorators and let the SDK handle the heavy lifting.



In this example, we're creating a document management server with two core tools: 
one to read documents and another to update them. 
All documents exist in memory as a simple dictionary where keys are document IDs and values are the content.

Setting Up the MCP Server
The Python MCP SDK makes server creation straightforward. You can initialize a server with just one line:

from mcp.server.fastmcp import FastMCP

mcp = FastMCP("DocumentMCP", log_level="ERROR")
Your documents can be stored in a simple dictionary structure:

docs = {
    "deposition.md": "This deposition covers the testimony of Angela Smith, P.E.",
    "report.pdf": "The report details the state of a 20m condenser tower.",
    "financials.docx": "These financials outline the project's budget and expenditures",
    "outlook.pdf": "This document presents the projected future performance of the system",
    "plan.md": "The plan outlines the steps for the project's implementation.",
    "spec.txt": "These specifications define the technical requirements for the equipment"
}
Tool Definition with Decorators
The SDK uses decorators to define tools. 
Instead of writing JSON schemas manually, you can use Python type hints and field descriptions. 
The SDK automatically generates the proper schema that Claude can understand.

Creating a Document Reader Tool
The first tool reads document contents by ID. Here's the complete implementation:

@mcp.tool(
    name="read_doc_contents",
    description="Read the contents of a document and return it as a string."
)
def read_document(
    doc_id: str = Field(description="Id of the document to read")
):
    if doc_id not in docs:
        raise ValueError(f"Doc with id {doc_id} not found")
    
    return docs[doc_id]

The decorator specifies the tool name and description, 
while the function parameters define the required arguments. 
The Field class from Pydantic provides argument descriptions that help Claude understand what each parameter expects.

Building a Document Editor Tool
The second tool performs simple find-and-replace operations on documents:

@mcp.tool(
    name="edit_document",
    description="Edit a document by replacing a string in the documents content with a new string."
)
def edit_document(
    doc_id: str = Field(description="Id of the document that will be edited"),
    old_str: str = Field(description="The text to replace. Must match exactly, including whitespace."),
    new_str: str = Field(description="The new text to insert in place of the old text.")
):
    if doc_id not in docs:
        raise ValueError(f"Doc with id {doc_id} not found")
    
    docs[doc_id] = docs[doc_id].replace(old_str, new_str)

This tool takes three parameters: the document ID, the text to find, and the replacement text. 
The implementation includes error handling for missing documents and performs a straightforward string replacement.

Key Benefits of the SDK Approach

No manual JSON schema writing required
Type hints provide automatic validation
Clear parameter descriptions help Claude understand tool usage
Error handling integrates naturally with Python exceptions
Tool registration happens automatically through decorators
The MCP Python SDK transforms tool creation from a complex schema-writing exercise into simple Python function definitions. 
This approach makes it much easier to build and maintain MCP servers while ensuring Claude receives properly formatted tool specifications.

---------------------------------------s
SERVER INSPECTOR

TO SEE server is working

Activate environment
run
mcp dev mcp_server.py
we will get 
Starting MCP inspector...
Proxy server listening on 6277
MCP Inspector is up and running at http://127.0.0.1.6274

on clicking the link http://127.0.0.1.6274, 
MCP Inspector opens

connect button : starts mcp server
we will see 
resources, prompts, tools, ping, sampling, roots

if we click on tools, then 
click on list tools
we will see list of tools that we created like

read_doc read the contents of a document 
edit_document edit a document by replacing a string

click on read_doc toll to manually invoke to check it works as expected.
and put doc_id
and click on run tool
we get 
run tool : Success

click on edit_document toll to manually invoke to check it works as expected.
and put doc_id, old_str, new_str
and click on run tool
we get 
 tool Result: Success

SERVER INSPECTOR : 
debug a MCP server, without wiring the MCP server with the actual application.


When building MCP servers, you need a way to test your functionality without connecting to a full application. 
The Python MCP SDK includes a built-in browser-based inspector that lets you debug and test your server in real-time.

Starting the Inspector
First, make sure your Python environment is activated (check your project's README for the exact command). Then run the inspector with:

mcp dev mcp_server.py
This starts a development server and gives you a local URL, typically something like http://127.0.0.1:6274. 
Open this URL in your browser to access the MCP Inspector.

Using the Inspector Interface
The inspector interface is actively being developed, so it may look different when you use it. 
However, the core functionality remains consistent. Look for these key elements:

A Connect button to start your MCP server
Navigation tabs for Resources, Tools, Prompts, and other features
A tools listing and testing panel
Click the Connect button first to initialize your server. You'll see the connection status change from "Disconnected" to "Connected".

Testing Your Tools
Navigate to the Tools section and click "List Tools" to see all available tools from your server. 
When you select a tool, the right panel shows its details and input fields.

For example, to test a document reading tool:

Select the read_doc_contents tool
Enter a document ID (like "deposition.md")
Click "Run Tool"
Check the results for success and expected output
The inspector shows both the success status and the actual returned data, making it easy to verify your tool works correctly.

Testing Tool Interactions
You can test multiple tools in sequence to verify complex workflows. For instance, after using an edit tool to modify a document, 
immediately test the read tool to confirm the changes were applied correctly.

The inspector maintains your server state between tool calls, 
so edits persist and you can verify the complete functionality of your MCP server.

Development Workflow
The MCP Inspector becomes an essential part of your development process. Instead of writing separate test scripts or 
connecting to full applications, you can:

Quickly iterate on tool implementations
Test edge cases and error conditions
Verify tool interactions and state management
Debug issues in real-time
This immediate feedback loop makes MCP server development much more efficient and helps catch issues early in the development process.

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
## above for MCP Server
## below for MCP Client
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Implementing a MCP client

mcp_client.py

In above file, we are making a mcp client class, which wraps Client session

MCP Client: Custom class we are authoring to make using the session easier, 
Client Session: Actual Connection to the MCP server.

Client session is a part of python SDK, so this session gives connection to outside server

client session requires resource clean up
so some clean up code in mcp_client.py
like async def cleanup(), when we close down the server or dont need the resource

so we use client session wrapped in class for clean up resources.

what does mcp client does

our CLI code is using the client to get a list of tools to pass to claude.
our CLI code is using the client to call a tool.

client exposes some functionality, 
that belongs to server to rest of the code base

implement tools functions
list_tools
call_tools

async def list_tools(self) -> list[types.Tool]:
   result =await self.session().list_tools()
   return result.tools

above codes, gets access to session, actual connection to mcp server,
calls built in functions, to get defintion or lists of all tools implemented by the server
and return result as tools.

async def call_tool(self, tool_name: str, tool_input: dict) ->
      types.CallToolResult | None:
      return await self.session().call_tool(tool_name, tool_input)

# testing block - to test above functions
async def main():
   async with MCPClient(
     command ="uv", args=["run", "mcp_server.py"]
    ) as client:
    pass

above codes, 
run mcp_client file , it forms a connection to mcp server

# testing block - to test above functions
async def main():
   async with MCPClient(
     command ="uv", args=["run", "mcp_server.py"]
    ) as client:
        result =await_client.list_tools()
        print(result)

above codes, starts a copy of mcp server, then it gets list of all tools and print the result

terminal: uv run mcp_client.py

output, we get list of tools definition/lists, with tool schema, which will be passed to claude

we have list tools and call tools functionality in code


terminal : uv run main.py
> what is the contents of the report.pdf document?
we will get response

mcp client allows us to access the functionality implemented inside the MCP server.
-----
Now that we have our MCP server working, it's time to build the client side. 
The client is what allows our application code to communicate with the MCP server and access its functionality.

Understanding the Client Architecture
In most real-world projects, you'll either implement an MCP client or an MCP server - not both. 
We're building both in this project just so you can see how they work together.


The MCP client consists of two main components:

MCP Client - A custom class we create to make using the session easier
Client Session - The actual connection to the server (part of the MCP Python SDK)

The client session requires careful resource management - we need to properly clean up connections when we're done. 
That's why we wrap it in our own class that handles all the cleanup automatically.

How the Client Fits Into Our Application
Remember our application flow diagram? The client is what enables our code to interact with the MCP server at two key points:


Our CLI code uses the client to:

Get a list of available tools to send to Claude
Execute tools when Claude requests them
Implementing Core Client Functions
We need to implement two essential functions: list_tools() and call_tool().

** List Tools Function
This function gets all available tools from the MCP server:

async def list_tools(self) -> list[types.Tool]:
    result = await self.session().list_tools()
    return result.tools
It's straightforward - we access our session (the connection to the server), call the built-in list_tools() method, 
and return the tools from the result.

** Call Tool Function
This function executes a specific tool on the server:

async def call_tool(
    self, tool_name: str, tool_input: dict
) -> types.CallToolResult | None:
    return await self.session().call_tool(tool_name, tool_input)

We pass the tool name and input parameters (provided by Claude) to the server and return the result.

Testing the Client
The client file includes a simple test harness at the bottom. You can run it directly to verify everything works:

uv run mcp_client.py
This will connect to your MCP server and print out the available tools. You should see output showing your tool definitions, 
including descriptions and input schemas.

Putting It All Together
Once the client functions are implemented, you can test the complete flow by running your main application:

uv run main.py
Try asking: "What is the contents of the report.pdf document?"

Here's what happens behind the scenes:

Your application uses the client to get available tools
These tools are sent to Claude along with your question
Claude decides to use the read_doc_contents tool
Your application uses the client to execute that tool
The result is returned to Claude, who then responds to you
The client acts as the bridge between your application logic and the MCP server's functionality, making it easy to integrate powerful
 tools into your AI workflows.

--------
Defining Resources

Resources in MCP servers allow you to expose data to clients, similar to GET request handlers in a typical HTTP server. 
They're perfect for scenarios where you need to fetch information rather than perform actions.


here, i want that users can mention adocumnet by writing out
"@doc_name" by
typing "@" should show a list of all the available documents
when a document is mentioned, its contents should be automatically injected into the prompt.

idea is not to reply on claude to use any tools, to fetch context from the document.

here 2 functions:
when a user types an "@" then we need the mcp server to give us a list of documents.
and
when a message contains a document mentioned, then we need the MCP server to give us the contents of the Whole document.

Resources:
1. Allows the MCP server to expose data to the client.
   one resource for one functionality
2. Similar to GET request handlers in a typical HTTP server
3. can return any type of data -strings, JSON, Binary, etc
   we set the "mime_type" to give the client a hint as to what data we are returnig
4. Two types- direct, templated


# Direct resource - for static resources (URI doesnt contain any params)

  @mcp.resource(
   "docs://documents", ## URI
    mime_type="application/json")
   def list_docs():
   ## return a list of document names

## Templated resource
  @mcp.resource(
   "docs://documents/{doc_id}", ## URI
    mime_type ="text/plain"
    )
  def fetch_doc(doc_id:str):
  # return the contents of a doc 
here, the URI contains one or more params, the python SDK parses these and passes them as args to ur function.

in terminal run : mcp uv run mcp dev mcp_server.py

starting MCP inspector...
Proxy server listening on port 6277
MCP Inspector is up and running at http://127.0.0.1.6274

click on connect
resources
list Resources -
list resource templates

click on list resources section, on url ( docs://documents/) - we see result
click on list resources templates section, on url (fetch_doc), enter the doc_id


Understanding Resources Through an Example
Let's say you want to build a document mention feature where users can type @document_name to reference files. This requires two operations:

Getting a list of all available documents (for autocomplete)
Fetching the contents of a specific document (when mentioned)

When a user mentions a document, your system automatically injects the document's contents into the prompt sent to Claude, 
eliminating the need for Claude to use tools to fetch the information.

How Resources Work
Resources follow a request-response pattern. When your client needs data, it sends a ReadResourceRequest with a URI to 
which resource it wants. The MCP server processes this request and returns the data in a ReadResourceResult.

The flow looks like this: your code requests a resource from the MCP client, which forwards the request to the MCP server. 
The server processes the URI, runs the appropriate function, and returns the result.

Types of Resources
There are two types of resources:

Direct Resources
Direct resources have static URIs that never change. They're perfect for operations that don't need parameters.

@mcp.resource(
    "docs://documents",
    mime_type="application/json"
)
def list_docs() -> list[str]:
    return list(docs.keys())

Templated Resources
Templated resources include parameters in their URIs. 
The Python SDK automatically parses these parameters and passes them as keyword arguments to your function.

@mcp.resource(
    "docs://documents/{doc_id}",
    mime_type="text/plain"
)
def fetch_doc(doc_id: str) -> str:
    if doc_id not in docs:
        raise ValueError(f"Doc with id {doc_id} not found")
    return docs[doc_id]


Implementation Details
Resources can return any type of data - strings, JSON, binary data, etc. 
Use the mime_type parameter to give clients a hint about what kind of data you're returning:

"application/json" for structured data
"text/plain" for plain text
"application/pdf" for binary files
The MCP Python SDK automatically serializes your return values. You don't need to manually convert objects to JSON strings - 
just return the data structure and let the SDK handle serialization.

Testing Your Resources
You can test resources using the MCP Inspector. Start your server with:

uv run mcp dev mcp_server.py
Then connect to the inspector in your browser. You'll see two sections:

Resources - Lists your direct/static resources
Resource Templates - Lists your templated resources


Click on any resource to test it. For templated resources, you'll need to provide values for the parameters. 
The inspector shows you the exact response structure your client will receive, including the MIME type and serialized data.

Resources provide a clean way to expose read-only data from your MCP server, 
making it easy for clients to fetch information without the complexity of tool calls.

---
Accessing Resources

client needs the ability to requests these resources

open mcp_client.py 

from pydantic import AnyUrl
import json

async def read_resource(self, uri:str) ->Any:
# TODO : Read a resource, parse the contents and return it
result=await self.session().read_resource(AnyUrl(uri))
resource =result.contents[0]

if instance(resource, types.TextResouurceContents):
  if resource.mimeType =='application/json":
     return json.loads(resource.text)

 return resource.text

mime type is type of data

terminal : mcp uv run main.py

>> i see the list of resources , check by arrow key

> what is in (use @), we see list of resources, use arrow keys to scroll

use space to insert resource, we want

the content of the documnet is send to the claude inside the prompt.

here, claude doesnt need tool to read the contents of the documnet

we use resurces to expose some amount of information from our mcp server

------
Accessing Resources

Implementing Resource Reading
To enable resource access in your MCP client, you need to implement a read_resource function. First, add the necessary imports:

import json
from pydantic import AnyUrl
The core function makes a request to the MCP server and processes the response based on its MIME type:

async def read_resource(self, uri: str) -> Any:
    result = await self.session().read_resource(AnyUrl(uri))
    resource = result.contents[0]
    
    if isinstance(resource, types.TextResourceContents):
        if resource.mimeType == "application/json":
            return json.loads(resource.text)
    
    return resource.text
Understanding the Response Structure
When you request a resource, the server returns a result with a contents list. 
We access the first element since we typically only need one resource at a time. The response includes:

The actual content (text or data)
A MIME type that tells us how to parse the content
Other metadata about the resource
Content Type Handling
The function checks the MIME type to determine how to process the content:

If it's application/json, parse the text as JSON and return the parsed object
Otherwise, return the raw text content
This approach handles both structured data (like JSON) and plain text documents seamlessly.

Testing Resource Access
Once implemented, you can test the resource functionality through your CLI application. 
When you type "@" followed by a resource name, the system will:

Show available resources in an autocomplete list
Let you select a resource using arrow keys and space
Include the resource content directly in your prompt
Send everything to the AI model without requiring additional tool calls
This creates a much smoother user experience compared to having the AI model make separate tool calls to 
access document contents. The resource content becomes part of the initial context, allowing for immediate responses about the data.


-------
Defining Prompts

Users can ask claude to 'format' a document as markdown as format is a command
User will initiate the process by typing a "/" to list out possible commands
User will specify the ID of the document to format
Claude should read the document's contents, then print a markdown -formatted version


define prompts inside the server, customed tailor to whatever is served by the mcp server.

define a prompt inside mcp server
user and assisted messages

mcp server.py - define prompts/ resources in server.py 

@mcp.prompt(
    name="format",
    description="Rewrites the contents of the document in Markdown Format."
)
def format_documnet(
   doc_id:str=Field(description ="ID of the document to format")
) ->list[base.Message]:
   prompt=f"""
   your goal is to reformat a document to be written with markdown syntax.
   The id of the document you need to format is:
   <document_id>
   {doc_id}
   </document_id>
   Add in headers, bullets points, tables etc as necessary, feel free to add in 
   use the edit_document tool to edit the document, after the document has been 
   """

   return [base.UserMessage(prompt)]

in terminal : mcp uv run mcp dev mcp_server.py

go in the link , provided by the mcp inspector
click on connect
then in prompts
list prompts
format is available 

provide doc_id
get prompt

we get prompt, send to claude, to get appropriate result

by this: 
prompts are well tested, evalauted , specialised for a particular use case.

--
Prompts in MCP servers let you define pre-built, high-quality instructions that clients can use instead of 
writing their own prompts from scratch. Think of them as carefully crafted templates that give better results
 than what users might come up with on their own.

Why Use Prompts?
Here's the key insight: users can already ask Claude to do most tasks directly. For example, a user could type 
"reformat the report.pdf in markdown" and get decent results. But they'll get much better results if you provide a 
thoroughly tested, specialized prompt that handles edge cases and follows best practices.

As the MCP server author, you can spend time crafting, testing, and evaluating prompts that work consistently across 
different scenarios. Users benefit from this expertise without having to become prompt engineering experts themselves.


Building a Format Command
Let's implement a practical example: a format command that converts documents to markdown. Users will type /format doc_id and get 
back a professionally formatted markdown version of their document.

The workflow looks like this:

User types / to see available commands
They select format and specify a document ID
Claude uses your pre-built prompt to read and reformat the document
The result is clean markdown with proper headers, lists, and formatting
Defining Prompts
Prompts use a similar decorator pattern to tools and resources:

@mcp.prompt(
    name="format",
    description="Rewrites the contents of the document in Markdown format."
)
def format_document(
    doc_id: str = Field(description="Id of the document to format")
) -> list[base.Message]:
    prompt = f"""
Your goal is to reformat a document to be written with markdown syntax.

The id of the document you need to reformat is:
<document_id>
{doc_id}
</document_id>

Add in headers, bullet points, tables, etc as necessary. Feel free to add in structure.
Use the 'edit_document' tool to edit the document. After the document has been reformatted...
"""
    
    return [
        base.UserMessage(prompt)
    ]
The function returns a list of messages that get sent directly to Claude. You can include multiple user and assistant messages to 
create more complex conversation flows.

Testing Your Prompts
Use the MCP Inspector to test your prompts before deploying them:


The inspector shows you exactly what messages will be sent to Claude, including how variables get interpolated into your prompt template. 
This lets you verify the prompt looks correct before users start relying on it.

Key Benefits
Consistency - Users get reliable results every time
Expertise - You can encode domain knowledge into prompts
Reusability - Multiple client applications can use the same prompts
Maintenance - Update prompts in one place to improve all clients
Prompts work best when they're specialized for your MCP server's domain. A document management server might have prompts for formatting, 
summarizing, or analyzing documents. A data analysis server might have prompts for generating reports or visualizations.

The goal is to provide prompts that are so well-crafted and tested that users prefer them over writing their own instructions from scratch.

----------
access prompts/resources in client.py

mcp clients helps to list prompts and get prompts(with arguements)

async def list_prompts(self) -> list[types.Prompt]:
    result = await self.session().list_prompts()
    return result.prompts

async def get_prompt(self, prompt_name, args: dict[str, str]):
    result = await self.session().get_prompt(prompt_name, args)
    return result.messages

in terminal : mcp uv run main.py
> 
we get , when we use /, 
we get format - to rewrite the contents in mArkdown format
format is the name of the prompt to invoke
format space, we get to select doc_id, 
select doc_id and enter


claude has now instructions to format the documnet and also provided doc_id

fetch document context by git documnent tool
claude responses after formating documnets

The final step in building our MCP client is implementing prompt functionality. This allows us to list all available prompts from 
the server and retrieve specific prompts with variables filled in.

Implementing List Prompts
The list_prompts method is straightforward. It calls the session's list prompts function and returns the prompts:

async def list_prompts(self) -> list[types.Prompt]:
    result = await self.session().list_prompts()
    return result.prompts

Getting Individual Prompts
The get_prompt method is more interesting because it handles variable interpolation. When you request a prompt, you provide arguments 
that get passed to the prompt function as keyword arguments:

async def get_prompt(self, prompt_name, args: dict[str, str]):
    result = await self.session().get_prompt(prompt_name, args)
    return result.messages
For example, if your server has a format_document prompt that expects a doc_id parameter, the arguments dictionary would contain 
{"doc_id": "plan.md"}. This value gets interpolated into the prompt template.

Testing Prompts in Action
Once implemented, you can test prompts through the CLI. When you type a slash (/), available prompts appear as commands. 
Selecting a prompt like "format" will prompt you to choose from available documents.

After selecting a document, the system sends the complete prompt to Claude. The AI receives both the formatting
 instructions and the document ID, then uses available tools to fetch and process the content.

How Prompts Work


Prompts define a set of user and assistant messages that clients can use. They should be high-quality, 
well-tested, and relevant to your MCP server's purpose. The workflow is:

Write and evaluate a prompt relevant to your server's functionality
Define the prompt in your MCP server using the @mcp.prompt decorator
Clients can request the prompt at any time
Arguments provided by the client become keyword arguments in your prompt function
The function returns formatted messages ready for the AI model
This system creates reusable, parameterized prompts that maintain consistency while allowing customization
through variables. It's particularly useful for complex workflows where you want to ensure the AI receives properly structured instructions every time.


The final step in building our MCP client is implementing prompt functionality. This allows us to list all available prompts from the server and 
retrieve specific prompts with variables filled in.

Implementing List Prompts
The list_prompts method is straightforward. It calls the session's list prompts function and returns the prompts:

async def list_prompts(self) -> list[types.Prompt]:
    result = await self.session().list_prompts()
    return result.prompts

Getting Individual Prompts
The get_prompt method is more interesting because it handles variable interpolation. 
When you request a prompt, you provide arguments that get passed to the prompt function as keyword arguments:

async def get_prompt(self, prompt_name, args: dict[str, str]):
    result = await self.session().get_prompt(prompt_name, args)
    return result.messages
For example, if your server has a format_document prompt that expects a doc_id parameter, 
the arguments dictionary would contain {"doc_id": "plan.md"}. This value gets interpolated into the prompt template.

Testing Prompts in Action
Once implemented, you can test prompts through the CLI. 
When you type a slash (/), available prompts appear as commands. 
Selecting a prompt like "format" will prompt you to choose from available documents.

After selecting a document, the system sends the complete prompt to Claude. 
The AI receives both the formatting instructions and the document ID, then uses available tools to fetch and process the content.

How Prompts Work


Prompts define a set of user and assistant messages that clients can use. 
They should be high-quality, well-tested, and relevant to your MCP server's purpose. The workflow is:

Write and evaluate a prompt relevant to your server's functionality
Define the prompt in your MCP server using the @mcp.prompt decorator
Clients can request the prompt at any time
Arguments provided by the client become keyword arguments in your prompt function
The function returns formatted messages ready for the AI model
This system creates reusable, parameterized prompts that maintain consistency while allowing customization through variables. 
It's particularly useful for complex workflows where you want to ensure the AI receives properly structured instructions every time.

--

You're building an MCP client to connect your application to an MCP server. What are the two main components you need?


 An MCP Client class and a Client Session

----
Your MCP client needs to find out what tools are available from an MCP server. What message type should it send?

 ListToolsRequest

---

You've built an MCP server and want to test if your tools work correctly before connecting to a full application. What's the easiest way to do this?

Use the built-in MCP Inspector with `mcp dev mcp_server.py`

--------

You're building a chat app where users ask Claude about their GitHub data. Without MCP, what's the main problem you'd face?

You'd have to write and maintain all the GitHub tool code yourself

---------
You're deciding how to implement a new feature in your MCP server. Users should be able to click a button to trigger a "summarize document" workflow. 
Which MCP primitive should you use?

 Resources - because you need to fetch document data
 Functions - because it involves processing
 Prompts - because users control when to start the workflow
 Tools - because the AI needs new capabilities

prompts/ functions
-----------
You're using the Python MCP SDK to create a tool that reads files. What's the easiest way to define this tool?

Use the @mcp.tool() decorator on a Python function
--------
You want to create a resource that fetches different documents based on their ID, like docs://documents/report.pdf. What type of resource should you use?

A templated resource with parameters in the URI
----------------------------------

Now that we've built our MCP server, let's review the three core server primitives and understand when to use each one. 
The key insight is that each primitive is controlled by a different part of your application stack.


Tools: Model-Controlled
Tools are controlled entirely by Claude. The AI model decides when to call these functions, and the results are used directly by Claude to accomplish tasks.

Tools are perfect for giving Claude additional capabilities it can use autonomously. 
When you ask Claude to "calculate the square root of 3 using JavaScript," it's Claude that decides to use a JavaScript execution tool
to run the calculation.

Resources: App-Controlled
Resources are controlled by your application code. Your app decides when to fetch resource data and how to use it - typically for UI elements 
or to add context to conversations.

In our project, we used resources in two ways:

Fetching data to populate autocomplete options in the UI
Retrieving content to augment prompts with additional context
Think of the "Add from Google Drive" feature in Claude's interface - the application code determines which documents to show and handles 
injecting their content into the chat context.

Prompts: User-Controlled
Prompts are triggered by user actions. Users decide when to run these predefined workflows through UI interactions like button clicks, 
menu selections, or slash commands.

Prompts are ideal for implementing workflows that users can trigger on demand. In Claude's interface, those workflow buttons below the 
chat input are examples of prompts - predefined, optimized workflows that users can start with a single click.

Choosing the Right Primitive
Here's a quick decision guide:

Need to give Claude new capabilities? Use tools
Need to get data into your app for UI or context? Use resources
Want to create predefined workflows for users? Use prompts
You can see all three primitives in action in Claude's official interface. 
The workflow buttons demonstrate prompts, the Google Drive integration shows resources in action, and when Claude executes code or performs calculations,
 it's using tools behind the scenes.

These are high-level guidelines to help you choose the right primitive for your specific use case. 
Each serves a different part of your application stack - tools serve the model, resources serve your app, and prompts serve your users.

####################################################################################################
####################################################################################################

Model Context Protocol: Advanced Topics

This course examines advanced features and implementation patterns for Model Context Protocol (MCP) development, focusing on server-client communication, 
transport mechanisms, and production deployment considerations. You'll explore sophisticated MCP capabilities including sampling for AI model integration,
notification systems, file system access control, and the technical details of different transport protocols.

What you'll learn
Sampling implementation - Understand how MCP servers can request language model calls through connected clients, 
including the architecture that shifts AI costs and complexity from server to client

Progress and logging notifications - Learn to implement real-time feedback systems using context objects, 
logging callbacks, and progress reporting for long-running operations

Roots-based file access - Explore permission systems that grant MCP servers access to specific directories while 
providing security boundaries and enabling user-friendly file discovery

JSON message architecture - Examine the complete MCP message specification, distinguishing between 
request-result pairs and notification messages, and understanding bidirectional communication patterns

Stdio transport mechanisms - Understand how MCP clients and servers communicate through standard input/output streams, 
including the required initialization handshake sequence

StreamableHTTP transport implementation - Learn how Server-Sent Events (SSE) enable server-to-client communication over HTTP, 
including session management and dual-connection architectures

HTTP transport limitations - Discover how configuration flags affect functionality, particularly regarding server-initiated requests and 
streaming capabilities

Production scaling considerations - Understand when to use stateless HTTP for horizontal scaling with load balancers and the trade-offs 
between stateful and stateless server configurations

Transport selection criteria - Learn to choose appropriate transport methods based on deployment requirements, functionality needs, 
and scaling constraints

Prerequisites
Experience with Python development and async programming patterns
Familiarity with JSON message formats and HTTP protocols
Basic knowledge of Server-Sent Events (SSE)

Who this course is for
Developers working with Model Context Protocol implementations
Engineers building MCP servers and clients
----------------------------

Sampling : Allows MCP servers to make calls to Claude through a client

Log and Progress Notifications : Give users better feedback during long-running operations.

Roots : Direct local MCP servers to specific files/folders
Messages Format: Understands how MCP clients and servers communicate

StandardIO transport : see how the stdio transport works

Streamable HTTP Transport : explore how remotely hosted MCP servers work

---------

CORE MCP FEATURES

1. SAMPLING

option 1: mcp server reach directly to claude/llm and summarize the results got from searches, mcp server direct access to language models
but, this brings complexity to MCP Server, we need to write some code to allow the MCP server to make a request off to a claude, get the response back, 
extract the generated text, so , on, we also have to ensure MCP SERVER has a API key to access claude/ LLM.

option 2: MCP Server make use of the technique called sampling, in sampling, MCP server ask the client, to run a prompt on his behalf, 
mcp server is going to write up a prompt, send it off to the MCP client, and say feed this prompt into claude/llm for me, 
the mcp client takes prompt and send it to claude/llm, it will get a response back, and send the result of the text generation to the mcp server, 
this approach moves the complexity over to the MCP client, 
as mcp client(next js app) already has connection with the, claude, so it can make a request on behalf of the MCP server, 
and the mcp server doesnot need an API key to access claude at all, 

and if it is public mcp server, then dont need to worry about paying for tokens generated on behalf of someone else making use of the server.


sampling : allows a MCP server, to ask client, whichever clients connects, to run the llm to generate response, 

sampling : most useful, when in publicly accessible mcp server, 

inside the server, inside the tool function, call create_message(), where u pass a list of messages,
which to be handed it to the claude, this list of messages be formulated into a request, and sent to client, 

in client side, we have sampling callback

sampling callback: it receives the messages that were sent from the server, inside the callback, we can call the claude/LLM model, 
generate a text, return a create_message result, 

------------------------------
Sampling allows a server to access a language model like Claude through a connected MCP client. Instead of the server directly calling Claude,
 it asks the client to make the call on its behalf. This shifts the responsibility and cost of text generation from the server to the client.

The Problem Sampling Solves
Imagine you have an MCP server with a research tool that fetches information from Wikipedia. After gathering all that data, you need to summarize 
it into a coherent report. You have two options:



Option 1: Give the MCP server direct access to Claude. The server would need its own API key, handle authentication, manage costs, and
 implement all the Claude integration code. This works but adds significant complexity.



Option 2: Use sampling. The server generates a prompt and asks the client "Could you call Claude for me?" The client, which already has a 
    connection to Claude, makes the call and returns the results.

How Sampling Works
The flow is straightforward:

Server completes its work (like fetching Wikipedia articles)
Server creates a prompt asking for text generation
Server sends a sampling request to the client
Client calls Claude with the provided prompt
Client returns the generated text to the server
Server uses the generated text in its response
Benefits of Sampling
Reduces server complexity: The server doesn't need to integrate with language models directly
Shifts cost burden: The client pays for token usage, not the server
No API keys needed: The server doesn't need credentials for Claude
Perfect for public servers: You don't want a public server racking up AI costs for every user
Implementation
Setting up sampling requires code on both sides:

Server Side
In your tool function, use the create_message function to request text generation:

@mcp.tool()
async def summarize(text_to_summarize: str, ctx: Context):
    prompt = f"""
    Please summarize the following text:
    {text_to_summarize}
    """
    
    result = await ctx.session.create_message(
        messages=[
            SamplingMessage(
                role="user",
                content=TextContent(
                    type="text",
                    text=prompt
                )
            )
        ],
        max_tokens=4000,
        system_prompt="You are a helpful research assistant",
    )
    
    if result.content.type == "text":
        return result.content.text
    else:
        raise ValueError("Sampling failed")
Client Side
Create a sampling callback that handles the server's requests:

async def sampling_callback(
    context: RequestContext, params: CreateMessageRequestParams
):
    # Call Claude using the Anthropic SDK
    text = await chat(params.messages)
    
    return CreateMessageResult(
        role="assistant",
        model=model,
        content=TextContent(type="text", text=text),
    )
Then pass this callback when initializing your client session:

async with ClientSession(
    read,
    write,
    sampling_callback=sampling_callback
) as session:
    await session.initialize()

When to Use Sampling
Sampling is most valuable when building publicly accessible MCP servers. 
You don't want random users generating unlimited text at your expense. 
By using sampling, each client pays for their own AI usage while still benefiting from your server's functionality.

The technique essentially moves the AI integration complexity from your server to the client, 
which often already has the necessary connections and credentials in place.

------------------------------------------------

SAMPLING WALKTHROUGH:

1. Initiating Sampling : on a server, during  a tool call, run the create_message() method, 
passing in some messages that u wish to send to a language model.

## Initiating Sampling Codes
    result = await ctx.session.create_message(
        messages=[
            SamplingMessage(
                role="user", content=TextContent(type="text", text=prompt)
            )
        ],
        max_tokens=4000,
        system_prompt="You are a helpful research assistant.",
    )
2. Sampling callbacks : on the client,u must implement a sampling callback, it will receive a list of messages provided by the server. 

## sampling callback 
async def sampling_callback(
    context: RequestContext, params: CreateMessageRequestParams
):

3. Message Formats: The list of messages provided by the server are formatted for communication in MCP, 
the individual messages arent guaranteed to be compatible with whatever LLM SDK u r using, 
for example, if u r using the Antropic SDK, write a conversion logic to turn the MCP Messages into a format compatible with the Antropic SDK, 

## message format codes

    for msg in input_messages:
        if msg.role == "user" and msg.content.type == "text":
            content = (
                msg.content.text
                if hasattr(msg.content, "text")
                else str(msg.content)
            )
            messages.append({"role": "user", "content": content})
        elif msg.role == "assistant" and msg.content.type == "text":
            content = (
                msg.content.text
                if hasattr(msg.content, "text")
                else str(msg.content)
            )
            messages.append({"role": "assistant", "content": content})

4. Returning generated text: after generating text with the LLM, we will return a CreateMessageResult, which contains the generated text, 
 
 # Call Claude using the Anthropic SDK
    text = await chat(params.messages)

    return CreateMessageResult(
        role="assistant",
        model=model,
        content=TextContent(type="text", text=text),
    )

5. Connecting the callback: the callback on the client needs to be passed into the ClientSession call.

 ## Connecting the callback
        async with ClientSession(
            read, write, sampling_callback=sampling_callback
        ## 

6. Getting the result:  After the client has generated and returned some text, it will be sent to the server,
we can do anything with the text like, 
use it as part of a workflow inur tool, 
decide to make another sampling call, 
return the generated_text

if result.content.type == "text":
        return result.content.text
    else:
        raise ValueError("Sampling failed")

------------------

Log and progress notifications: 


Logging and notifications: when it is enabled, we get progress meter and some log statements to go along, 
these are log and progress statements, that are being emited inside of the tool call of tool.

to start with the logging and progress notifications inside, of a tool function on our server, we will receive, 
the context arguement, which is automatically included as the 
last arguement to our tool function, on that context, object are a variety of different methods that allows us to either,
log information or report the progress, of this tool run bac over to the client, 
so we, get methods like info and report progress, 
anytime, u call these functions, a message will be automatically
sent back to the client, to make use of the log statements and the progress updates on the client, we will write code to put together 
a call back fucntion, they will be called , whenever we receive, a logging statement, from the server, 

we will also make a seperate callback, that will receives updates on progress from the server, we will 
then take the logging callback and pass it off to the client session and the progress callback and that gets passed off to the call tool fucntion, 
inside, of these callbacks, its upto u , how u report these log statements and the progress to user, 
-----
Logging and progress notifications are simple to implement but make a huge difference in user experience when working with MCP servers. 
They help users understand what's happening during long-running operations instead of wondering if something has broken.

When Claude calls a tool that takes time to complete - like researching a topic or processing data - users typically see nothing 
until the operation finishes. This can be frustrating because they don't know if the tool is working or has stalled.

With logging and progress notifications enabled, users get real-time feedback showing exactly what's happening behind the scenes. 
They can see progress bars, status messages, and detailed logs as the operation runs.

How It Works
In the Python MCP SDK, logging and progress notifications work through the Context argument that's automatically provided to your tool 
functions. This context object gives you methods to communicate back to the client during execution.

@mcp.tool(
    name="research",
    description="Research a given topic"
)
async def research(
    topic: str = Field(description="Topic to research"),
    *,
    context: Context
):
    await context.info("About to do research...")
    await context.report_progress(20, 100)
    sources = await do_research(topic)
    
    await context.info("Writing report...")
    await context.report_progress(70, 100)
    results = await generate_report(sources)
    
    return results
The key methods you'll use are:

context.info() - Send log messages to the client
context.report_progress() - Update progress with current and total values
Client-Side Implementation
On the client side, you need to set up callback functions to handle these notifications. 
The server emits these messages, but it's up to your client application to decide how to present them to users.

async def logging_callback(params: LoggingMessageNotificationParams):
    print(params.data)

async def print_progress_callback(
    progress: float, total: float | None, message: str | None
):
    if total is not None:
        percentage = (progress / total) * 100
        print(f"Progress: {progress}/{total} ({percentage:.1f}%)")
    else:
        print(f"Progress: {progress}")

async def run():
    async with stdio_client(server_params) as (read, write):
        async with ClientSession(
            read,
            write,
            logging_callback=logging_callback
        ) as session:
            await session.initialize()
            
            await session.call_tool(
                name="add",
                arguments={"a": 1, "b": 3},
                progress_callback=print_progress_callback,
            )
You provide the logging callback when creating the client session, 
and the progress callback when making individual tool calls. 
This gives you flexibility to handle different types of notifications appropriately.

Presentation Options
How you present these notifications depends on your application type:

CLI applications - Simply print messages and progress to the terminal
Web applications - Use WebSockets, server-sent events, or polling to push updates to the browser
Desktop applications - Update progress bars and status displays in your UI
Remember that implementing these notifications is entirely optional. You can choose to ignore them completely, 
show only certain types, or present them however makes sense for your application. They're purely user experience enhancements 
to help users understand what's happening during long-running operations.

===============
Notifications walk through

1. Tool Function receives context arguements: Tool functions automatically receive 'context' as their last arguement, 
this object has methods for logging and reporting progress to the client.

2. Create logs and Progress with context: Throughout ur tool function call, the info(), warning(), debug(), or error() methods 
to log different types of messages for the client, also call the report_progress() method to estimate the amount of remaining work for the tool call,

3. Define callbacks on the client : The client needs to define logging and progress callbacks, which will automatically be called 
whenever the server emits log or progress messages, these callbacks, should try to display logging and progress data to the user.

4. Pass callbacks to appropriate functions : make sure u provide the logging callback to the ClientSession and the progress callback 
to the call_tool() function,

============
roots

Roots are a way to grant MCP servers access to specific files and folders on your local machine. Think of them as a permission system 
that says "Hey, MCP server, you can access these files" - but they do much more than just grant permission.

The Problem Roots Solve
Without roots, you'd run into a common issue. 
Imagine you have an MCP server with a video conversion tool that takes a file path and converts an MP4 to MOV format.

When a user asks Claude to "convert biking.mp4 to mov format", Claude would call the tool with just the filename.
But here's the problem - Claude has no way to search through your entire file system to find where that file actually lives.

Your file system might be complex with files scattered across different directories. 
The user knows the biking.mp4 file is in their Movies folder, but Claude doesn't have that context.

## issue and solution, 
You could solve this by requiring users to always provide full paths, but that's not very user-friendly. 
Nobody wants to type out complete file paths every time.

Roots in Action
Here's how the workflow changes with roots:

User asks to convert a video file
Claude calls list_roots on mcp server to see what directories it can access
Claude calls read_dir on accessible directories to find the file
Once found, Claude calls the conversion tool with the full path
This happens automatically - users can still just say "convert biking.mp4" without providing full paths.

Security and Boundaries
Roots also provide security by limiting access. If you only grant access to your Desktop folder, the MCP server cannot access files in other locations
like Documents or Downloads.

When Claude tries to access a file outside the approved roots, it gets an error and can inform the user that the file isn't accessible from 
the current server configuration.

Implementation Details
The MCP SDK doesn't automatically enforce root restrictions - you need to implement this yourself. A typical pattern is to create a helper 
function like is_path_allowed() that:

Takes a requested file path
Gets the list of approved roots
Checks if the requested path falls within one of those roots
Returns true/false for access permission
You then call this function in any tool that accesses files or directories before performing the actual file operation.

Key Benefits
User-friendly - Users don't need to provide full file paths
Focused search - Claude only looks in approved directories, making file discovery faster
Security - Prevents accidental access to sensitive files outside approved areas
Flexibility - You can provide roots through tools or inject them directly into prompts
 Roots make MCP servers both more powerful and more secure by giving Claude the context it needs to find files while maintaining clear boundaries 
around what it can access.


we solve issue( why roots is important) , ( of providing full path, llm model not finding file)
as mcp server has 3 tools

1. convert_video
2. read_dir : return a list of files/folders in a specified directory.
3. list_roots: Return a list of roots: files/folders that the user has granted permission to read.
               The read_dir and convert_video tools are only allowed to work on files/folders contained in one of these roots.

to run the program
terminal: roots uv run main.py ~/Desktop
uv python terminal 
~/Desktop : means desktop is set as a root, inside the MCP server, so server can access desktop folders/files
 we can get desktop directory, click on it

provide prompt
> turn the biking.mp4 file into a mov file

couple of tools will be printed on the terminal, 
like
tool call :list_roots
tool call :read-dir

Arguements:
{ "path":"/Users/sgrider/Desktop"
}
and will find the biking.mp4 file on desktop,

Tool call: convert_video
Arguements:
{ "input_path":"/Users/sgrider/Desktop/biking.mp4",
   "format":"mov"
}

successfully converts the video into mov

first the claude/llm model will look over all the different files and folders that it can access by calling the list,

roots are intended to limit what files and folders mcp server can access, 


> Convert the ~/documents/swim.mp4 to mov
  response, claude cant access to above file
in above way, roots prevents access to all files and folders, 

ROOTS WALKTHROUGH

1. defining roots : a user will dictate which files/folder can be accessed by the MCP Server, 
   the program is set up to accept a list of CLI arguements, which are interpreted as paths that the user wants to allow access to,
   that list of paths is provided to the MCP client

   root_paths= sys.argv[1:]

2. Creating root objects : according to the MCP spec, all roots should have a URI that begins with,  file://
   this function takes the list of paths of that the user provided and turns them into root objects.

3. Roots Callback : the client doesnt immediately, provide the list of roots to the server, instead , the server can make a request to the client
    at some future point in time, we make a callback that will be executed when the server request the roots, the callback needs to return the list of roots 
    inside of a ListRootsResult object,  the callback is passed into the ClientSession 

4. Using the roots : on to the server, the server will use the roots in 2 scenarios:
   1 . Whenever a tool attempts to access a file or folder
   2. when a LLM needs to resolve a file or folder to a full path,
      think of when a user says " read the todox.text file, llm needs to figure out where the text file is and might do so by looking at the list of roots.
      To handle the second case, we can either define a tool that lists out the roots or inject them directly in a prompt.

5. Accessing the roots:
    Roots are accessed by calling 
    ctx.session.list_roots()
    this sends a message back to the client, which causes it to run the root-listing callback.

6. Authorizing Access 
   the MCP SDK does not attempt to limit what files or folders your tools attempt to read, u must implement that check by urself, 
   consider implementing a function like is_path_allowed, which will decide whether a path is accessible by comapring it to the list of roots.

7. Authorizing Access : Once u have put an authorization function like is_path_allowed , 
   use it throughout your tools to ensure the requested path is accessible.

----------------

JSON MESSAGES TYPE

Messages Format: clients n servers connect via JSON, we refer json as messages, and many types of messages defined in mcp spec,
each designed for a distinct purpose,  

streamable http transport : allows clients to connect to remotely hosted mcp servers

** if a llm connected to a mcp client,decides, to call a tool provided by the mcp server, the client will send a message over to the server, this  type of message is called a call tool request.
** the mcp server will then run the tool, and then put the result of the tool run into another message type called a call tool result, 


MCP SPECIFICATION

github.com/modelcontextprotocol/modelcontextprotocol


defines how mcp clients and servers should behave, 
defines all the different valid message types (written in typescript)


in github.com/modelcontextprotocol/modelcontextprotocol/schema/draft/schema.tx
above contains all the different message types, 

search call tool request

## below is code of call tool request 
export interface CallToolRequest extends Request{
method :"tools/call"
params: {
   name: String;
   arguemnts?:{[key :string] :unknown};
   };}

call tool request, it should have method, CallToolRequest and parameter object


MCP CLIENT -------- CALL TOOL REQUEST -----> MCP SERVER

MCP SERVER ------- CALL TOOL RESULT ------>  MCP CLIENT

CALL TOOL REQUEST
{"jsonrpc":"2.0",
 "id" :1,
 "method" :"tools/call"
 "params":{
 "name" :"add", "arguements":{"a" :5, "b":3}
 } }


Call Tool Result
{ "jsonrpc":"2.0",
   "id" :1,
   "result": {
   "content": [{"type":"text", "text":"8"}],
   "isError" :false
}}

jsonrpc is defined in below codes
export type JSONRPCMessage=
   | JSONRPCRequest
   | JSONRPCNotification
   | JSONRPCResponse
   | JSONRPCError;

export interface JSONRPCRequest extends Request{
    jsonrpc: typeof JSOBRPC_VERSION;
    id : RequestId;}

server messages, client messages

server messages -sent by server to client
export type ServerRequest =
  | PingRequest
  | CreateMessageRequest
  | ListRootsRequest
  | ElicitRequest;

export type ServerNotification =
  | CancelledNotification
  | ProgressNotification
  | LoggingMessageNotification
  | ResourceUpdatedNotification
  | ResourceListChangedNotification
  | ToolListChangedNotification

----

MCP (Model Context Protocol) uses JSON messages to handle communication between clients and servers.
 Understanding these message types is crucial for working with MCP, especially when dealing with different transport 
methods like the streamable HTTP transport.

Message Format
All MCP communication happens through JSON messages. Each message type serves a specific purpose - 
whether it's calling a tool, listing available resources, or sending notifications about system events.



Here's a typical example: when Claude needs to call a tool provided by an MCP server, the client sends a "Call Tool Request" message. 
The server processes this request, runs the tool, and responds with a "Call Tool Result" message containing the output.



MCP Specification
The complete list of message types is defined in the official MCP specification repository on GitHub. This specification is separate 
from the various SDK repositories (like Python or TypeScript SDKs) and serves as the authoritative source for how MCP should work.

The message types are written in TypeScript for convenience - not because they're executed as TypeScript code, but because TypeScript 
provides a clear way to describe data structures and types.

Message Categories
MCP messages fall into two main categories:



Request-Result Messages
These messages always come in pairs. You send a request and expect to get a result back:

Call Tool Request  Call Tool Result
List Prompts Request  List Prompts Result
Read Resource Request  Read Resource Result
Initialize Request  Initialize Result
Notification Messages
These are one-way messages that inform about events but don't require a response:

Progress Notification - Updates on long-running operations
Logging Message Notification - System log messages
Tool List Changed Notification - When available tools change
Resource Updated Notification - When resources are modified
Client vs Server Messages
The MCP specification organizes messages by who sends them:

Client messages include requests that clients send to servers (like tool calls) and notifications that clients might send.

Server messages include requests that servers send to clients and notifications that servers broadcast.

Why This Matters
Understanding that servers can send messages to clients is particularly important when working with different transport methods. 
Some transports, like the streamable HTTP transport, have limitations on which types of messages can flow in which directions.

The key insight is that MCP is designed as a bidirectional protocol - both clients and servers can initiate communication. 
This becomes crucial when you need to choose the right transport method for your specific use case.

-------
MCP TRANSPORTS

CLIENT AND SERVER communicate by exhanging JSON Objects, referred as messages, 
json can be transmitted in client and server by :
1. make a http request
2. use web sockets
3. postcard with JSON,and send it to someone and have them manually type that json into a server, 

in the mcp specification, the thing that we used to actully move JSON between a client and server is called transport.
when we develop mcp client and mcp server, commonly used transport is standard IO Transport.
the idea with this transport is that our client is going to launch, a server as a seperate process, to handle the process, 
and client can send the messages into the server by writing them into the server standard in channel,
and it can receive messages by watching the server standard out channel, 

standard io transport is that the communication between client and server can be easily initiated,at any point in time, 
so the client can communicate or send a message off to the server, by writing to standard in. and the server can communicate back to the client
 by writing to standard out.
but one disadvantage is that we can only make use of this transport when the client and the server are running on the same physical machine.

## connect to server directly from the terminal, without creating distinct client 
in terminal:
transport-stdio git:(main) uv run server.py
above command starts the server, and now the server is going to be listening to standard in and the writing any outgoing messages to standard out.
so
when i run a program, type in terminal, then i am writing to standard in of this running program, so i can paste json messages here, and execute,
 them and they will be received as input to the server,


## CREATING A SERVER
from mcp.server.fastmcp import FastMCP,Context
import asyncio
mcp= FastMCP(name ="DEMO SERVER" , log_level="ERROR")


## defining one tool
@mcp.tool()
async def add(a: int, b: int, ctx: Context) -> int:
   await ctx.info("add")
   await asyncio.sleep(2)
   await ctx.report_progress(89,100)
defining one tool and starting server with standard IO

** the first json message sent by the MCP Client is initialize request.
** when client first connects to the server, the mcp specialization says 
   we must send a sequence of three different messages back and forth.
   the very first message must always be an initialize request,
initialize request : {"jsonrpc":"2.0", "method":"initialize", "params":{"protocolVersion":"2024-11-05", "capabilities": {}, "clientInfo": {"name":"demo-client", "version":"1.0.0"}}, "id" :1}
initialize result:
initialize notification(from client to server): {"jsonrpc":"2.0","method":"notifications/initialized"}

initialize notification has no immediate response

after exchanging above messages, the connection to the mcp server is initialized, 
we can then run call tool requests, or prompt listing requests, 

call tool requests:called the add tool, with arguements,  
{"jsonrpc":"2.0", "method":"tools/call","params":{"_meta":{"progressToken":"abc123"},"name":"add","arguements":{"a":5, "b":3}},"id":3}

we get notification message

{"method":"notifications/message","params":{"level" :"info", "data":"preparing to add.."},"jsonrpc":"2.0"}

then progress message

{"method": "notificatiobs/progress", "params":{"progressToken":"abc123","progress":80.0, "total":100.0}, "jsonrpc":"2.0"}

then result

{"jsonrpc":"2.0", "id":3, "result":{"content":[{"type":"text, "text"="8}],"isError":"false}}

----
4 different scenarios, with these different transports

1. we handle initial request from the client to the server, 
2. transport needs to be able to handle sending response from the server to the client.
3. the server needs to be able to send an initial request to the client, (example :sampling)
4. client needs to be able to respond

implement standard IO Transport

1. client sending a call to request something, to implement this with a standard IO transport, write a standard in
2. MCP SERVER WILL receive the server, process it, formualte a response,
3. once mcp server has response, it would respond by just writing a message to standard out,
4. initial request to be sent from the client to the server, 
5. if server wants sampling, then server sents initial message off to the client, server needs to write standard out
6. client , to response, has to write a standard in

standard IO transport 
Advantage: at any point or time, the client and server can initiate communication , if both on same physical machine
           either one can send a request at any point at any time and expect to get back a response

in streamable http transport : client and server cant initiate communication at any point, at any time
------------
MCP clients and servers communicate by exchanging JSON messages, but how do these messages actually get transmitted? 
The communication channel used is called a transport, and there are several ways to implement this - from HTTP requests 
to WebSockets to even writing JSON on a postcard (though that last one isn't recommended for production use).

The Stdio Transport
When you're first developing an MCP server or client, the most commonly used transport is the stdio transport. 
This approach is straightforward: the client launches the MCP server as a subprocess and communicates through standard input and output streams.



Here's how it works:

Client sends messages to the server using the server's stdin
Server responds by writing to stdout
Either the server or client can send a message at any time
Only works when client and server run on the same machine
Seeing Stdio in Action
You can actually test an MCP server directly from your terminal without writing a separate client.
when you run a server with uv run server.py, it listens to stdin and writes responses to stdout. 
This means you can paste JSON messages directly into your terminal and see the server's responses immediately.

The terminal output shows the complete message exchange, including example messages for initialization and tool calls.

MCP Connection Sequence
Every MCP connection must start with a specific three-message handshake:


Initialize Request - Client sends this first
Initialize Result - Server responds with capabilities
Initialized Notification - Client confirms (no response expected)
Only after this handshake can you send other requests like tool calls or prompt listings.

Message Types and Flow
MCP supports various message types that flow in both directions:



The key insight is that some messages require responses (requests  results) while others don't (notifications). 
Both client and server can initiate communication at any time.

Four Communication Scenarios
With any transport, you need to handle four different communication patterns:



Client  Server request: Client writes to stdin
Server  Client response: Server writes to stdout
Server  Client request: Server writes to stdout
Client  Server response: Client writes to stdin
The beauty of stdio transport is its simplicity - either party can initiate communication at any time using these two channels.

Why This Matters
Understanding stdio transport is crucial because it represents the "ideal" case where bidirectional communication is seamless. 
When we move to other transports like HTTP, we'll encounter limitations where the server cannot always initiate requests to the client. 
The stdio transport serves as our baseline for understanding what full MCP communication looks like before we tackle the constraints of other 
transport methods.

For development and testing, stdio transport is perfect. For production deployments where client and server need to run on different machines, 
you'll need to consider other transport options with their own trade-offs.

============
streamable HTTP Transport

this transport allows us to send messages, between a client and a server over an http connection,
it enables remotely hosted mcp servers.
remote server be hosted at MCPServer.com
so, we can make a public server, anyone can connect to

some settings limit the functionality of mcp server, like limiting the type of messages, that, we can send from the server to the client
so functionality of server is limited, when using streamable http transport, 

NEXT JS app, connected to MCP server that implements a research tool, this server is using the streamable  HTTP Transport


from mcp.server.fastmcp import FastMCP
from mcp_server.tools.research import research

mcp =FastMCP( "mcp-server", )

mcp.add_tool(research)

def run():
   mcp.run(transport ="streamable-http")


above code will show call tool request, with progress bar, status updates, and full response, 


## but, when we add settings like
stateless_http=True, 

we wont see a progress bar and requests fails , and no response is seen

## but, when we add settings like
stateless_http=True, json_response=True

we wont see a progress bar , status bar and requests fails , and no response is seen 

------## codes
from mcp.server.fastmcp import FastMCP
from mcp_server.tools.research import research

mcp =FastMCP( "mcp-server", stateless_http=True, json_response=True )

mcp.add_tool(research)

def run():
   mcp.run(transport ="streamable-http")


here, in http streamable, the client can make a post request to the server, the server can respond with a response message to the client.

the client can make a post request off to the server and expect response from the server.
but 
if the server wants to initiate a request to the client, it wont happen with http requests, as the server doesnt know the address of the client 
and client might not be publicly available, 
and so some requests like sampling (create message request), listing routes (List Roots Result) , Progress notifications , 
logging notifications cant be issued from the server to the client, 


solution: 
---
The streamable HTTP transport enables MCP clients to connect to remotely hosted servers over HTTP connections. 
Unlike the standard I/O transport that requires both client and server on the same machine, this transport opens up possibilities for public 
MCP servers that anyone can access.



However, there's an important caveat: some configuration settings can significantly limit your MCP server's functionality.
 If your application works perfectly with standard I/O transport locally but breaks when deployed with HTTP transport, this is likely the culprit.



Configuration Settings That Matter
Two key settings control how the streamable HTTP transport behaves:

stateless_http - Controls connection state management
json_response - Controls response format handling
By default, both settings are false, but certain deployment scenarios may force you to set them to true. When enabled, 
these settings can break core functionality like progress notifications, logging, and server-initiated requests.

The HTTP Communication Challenge
To understand why these limitations exist, we need to review how HTTP communication works. In standard HTTP:



Clients can easily initiate requests to servers (the server has a known URL)
Servers can easily respond to these requests
Servers cannot easily initiate requests to clients (clients don't have known URLs)
Response patterns from client back to server become problematic


MCP Message Types Affected
This HTTP limitation impacts specific MCP communication patterns. The following message types become difficult to implement with plain HTTP:

Server-initiated requests: Create Message requests, List Roots requests
Notifications: Progress notifications, Logging notifications, Initialized notifications, Cancelled notifications
These are exactly the features that break when you enable the restrictive HTTP settings. Progress bars disappear, 
logging stops working, and server-initiated sampling requests fail.

The Streamable HTTP Solution
The streamable HTTP transport does provide a clever solution to work around HTTP's limitations, but it comes with trade-offs. 
When you're forced to use stateless_http=True or json_response=True, you're essentially telling the transport to operate within 
HTTP's constraints rather than working around them.



Understanding these limitations helps you make informed decisions about:

Which transport to use for different deployment scenarios
How to design your MCP server to gracefully handle HTTP constraints
When to accept reduced functionality for the benefits of remote hosting
The key is knowing that these restrictions exist and planning your MCP server architecture accordingly. 
If your application heavily relies on server-initiated requests or real-time notifications, you may need to reconsider 
your transport choice or implement alternative communication patterns.

-----------------

Streamable HTTP

some mcp functionality relies on  a server making a request to the client
with http, its challenging to allow a server to make a request to a client
in some scenarios, when we use stateless_http and json_response to True, then no progress bar, status , loggings messages are seen.

** the client must send an initialized request to the server, server replies with a result, and the client has to make a follow-up request 
with the initialized notifcation,
   at this time, the server considers the client to be connected, 

when we use http transport

initialized result that gets sent back from the server, is going to have a header inside the HTTP response, 
this header is the MCP session ID, 
mcp session id is the random string of numbers and letters that gets assigned to our client, which acts as an identity for our connection to our server, 
as soon as we receive that header, we are required to include it in all follow-up requests we make to the server,
this allows the server to identify our client, 
so we do initialization and get the sessionid 
this allows servers to make requests to the client,

after the initialization, client can optionally make a GET request to the MCP server, an include that session id inside the request, 
response, we get is SSE (Server sent Events), this is a kind of response that can be held open for an arbitary amount of time, once
this response has been established with the client, the server can then stream back with little bits of information, like individual messages to client

once this connection has beem made, by HTTP Transport,
this SSE response is what allows the server to send requests to the client.
http transport makes use of this long -lived SSE response and it is going to stream down messages that it wants to send to the client

sse response opens connection, it has a session id tied to it, so mcp server knows, which client, the connection belong to, so in future time, 
while the response is 
still running, the client might decide to make a call tool request to the server,
when it makes this request, its going to include its session id, as a header, then the mcp server, is going to open up a second SSE response, so, 
now, at this point in time, we have, 
2 seperate sse responses, 
first sse response is intended to be used for requests, that are going from the server to the client, 
second SSE response is used for messages that are related to this call tool request
the second SSE response is closed automatically, as soon as the call tool result message is sent, so this response closes automatically , very shortly , 
as soon as we get result, and the first  response, remains open for an arbitary amount of time, 
the goal here is to call a tool, 

when we do a calltoolrequest, technically, logging notifications, progress notifications are tied to call tool request, 
but most sdk , progress notification ends up being considered to be seperate from the incoming call tool request, 
so the progress notification gets send in  the first SSE response, and this remains open for a long time to allow the server to make
a request to the client.
and the logging message and the actual call tool result, is sent back in the response to the POST request or the 
GET tool request that was made, 


## CREATING A SERVER
from mcp.server.fastmcp import FastMCP,Context
import asyncio
mcp= FastMCP(name ="DEMO SERVER" , log_level="ERROR")


## defining one tool
@mcp.tool()
async def add(a: int, b: int, ctx: Context) -> int:
   await ctx.info("add")
   await asyncio.sleep(2)
   await ctx.report_progress(89,100)
 
above codes connected to a client in the browser, 
and this allows to make a handful of different requests 
to the server and then show exact response.

the process :
first initialize request, to get session ID , which uniquely identifies my client to the server for any future request,

response : success

cache-control: no-cache, no-transform
connection : keep-alive
content-type: text/event-stream
mcp-session-id: caa24535fef45t4t4fvdvd4445fdfgghhii(32 digits)
server :uvicorn
transfer-encoding :chunked
x-accel-buffering : no

stream :SSE connection established
event: message
data :{"jsonrpc":"2.0","id":1,"result

}

session id is only provided, when using the streamable HTTP Transport, it is not used with the standard IO transport, 
mcp-session-id , header begins with eaa.
the header is automatically taken and applied to all future requests,

in the next request, we notice mcp session id, has automatically been applied with the correct session id, 
from first request, 


First request is : Initialize request
second Request is : Initialize Notification
third : Get SSE (server initiated events)
fourth : Tool call (Add Function)

we can know make the GET request to the server, 
Get Request is to be held open for a long time, 
this is what allows the server to send a request to the client
at any point of time, it wishes, 
its going to be used for sampling or any kind of server initiated request, 
to make the request, 
click on server-initiated events(Get SSE) at bottom, right hand side, 
click on Start GET SSE , the request is also going to include, the session ID as well, 
so , after connection , the server can send a message to client for sampling, logging, progress notifications, 


fourth : Tool call (Add Function)

{"jsonrpc":"2.0",
"id":3, 
"method": "tools/call",
"params":{
  "name" :"add",
  "arguements":{
   "a" :5,
   "b":3}}}

Content-Type application/json
Accept: application/json, text/event-stream
mcp-session-id: eaa2345555dfgddetdgddg43 (32 digits, starts with eaa)


we get correct session id,

progress notification can be sent as a part of first response (SSE response for server to client requests)

we should see log statement and a call tool result, in response and the progress statement in bottom right box,

run it (call add tool)

we see 
response :success

cache-control: no-cache, no-transform
connection : keep-alive
content-type: text/event-stream
mcp-session-id: caa24535fef45t4t4fvdvd4445fdfgghhii(32 digits)
server :uvicorn
transfer-encoding :chunked
x-accel-buffering : no

stream :SSE connection established
event: message

data :{"method":"notifications/message","params":
{"level" :"info", "data":"preparing to add.."},"jsonrpc": "2.0"}

event: message

data :{"jsonrpc" : "2.0", "id":3 , "result": {"content":
[{"type": "text", "text" :"8"}]."isError" 

stream :SSE connection close

----
StreamableHTTP is MCP's solution to a fundamental problem: some MCP functionality requires the server to make requests to the client, 
but HTTP makes this challenging. Let's explore how StreamableHTTP works around this limitation and when you might need to break that workaround.

The Core Problem
Some MCP features like sampling, notifications, and logging rely on the server initiating requests to the client. However, 
HTTP is designed for clients to make requests to servers, not the other way around. StreamableHTTP solves this with a clever 
workaround using Server-Sent Events (SSE).

How StreamableHTTP Works
The magic happens through a multi-step process that establishes persistent connections between client and server.

Initial Connection Setup
The process starts like any MCP connection:

Client sends an Initialize Request to the server
Server responds with an Initialize Result that includes a special mcp-session-id header
Client sends an Initialized Notification with the session ID
This session ID is crucial - it uniquely identifies the client and must be included in all future requests.

The SSE Workaround
After initialization, the client can make a GET request to establish a Server-Sent Events connection. This creates a long-lived HTTP 
response that the server can use to stream messages back to the client at any time.

This SSE connection is the key to allowing server-to-client communication. The server can now send requests, 
notifications, and other messages through this persistent channel.

Tool Calls and Dual SSE Connections
When the client makes a tool call, things get more complex. The system creates two separate SSE connections:

Primary SSE Connection: Used for server-initiated requests and stays open indefinitely
Tool-Specific SSE Connection: Created for each tool call and closes automatically when the tool result is sent

Message Routing
Different types of messages get routed through different connections:

Progress notifications: Sent through the primary SSE connection
Logging messages and tool results: Sent through the tool-specific SSE connection


Configuration Flags That Break the Workaround
StreamableHTTP includes two important configuration options:

stateless_http
json_response
Setting these to True can break the SSE workaround mechanism. 
You might want to enable these flags in certain scenarios, 
but doing so limits the full MCP functionality that depends on server-to-client communication.

Key Takeaways
StreamableHTTP is more complex than other MCP transports because it has to work around HTTP's limitations. 
The SSE-based workaround enables full MCP functionality over HTTP, but understanding the dual-connection model is crucial for debugging and optimization.

When building MCP applications with StreamableHTTP, remember that session IDs are required for all requests after 
initialization, and the system automatically manages multiple SSE connections to handle different types of server-to-client communication.


-----------

State and the streamable http transport

stateless_http =True,
json_response =True, 

how above flags affect our MCP server, 

scenario to make above flags = True, 

when we make a MCP server and deploy it somewhere publicly, online, where anyone can connect, 
we connect it with out client
and then some other people connect to it with their own clients as well, 
likewise, server becomes popular, 
and many clients connect to it, 
at a certain point, running a single instance of our server on a single, machine, would probably not scale to the amount of traffic, that is incoming, 
so we might solve this , by allowing horizontal scalling, 
with horizontal scaling, we might run multiple copies of our server, and then gate access to them with a load balancer,
so any incoming request would be randomly routed to
one of these different servers, this would allows us to scale up to meet higher demands of traffic,
 


suppose, we are using load balancer, here, and wants to have two seperate connections to our mcp server from any given client, 
first, we want to have that running "get request" running at all times, so we can receive requests from the server to the client 
and our client also needs to make post requests and then receive a SSE resposne
with some number of messages inside of it as well, 

one client connecting to two different servers, 
the client makes an initial request Setting up that initial Git SSE response pipeline, so we then imagine
that we have got this response pending or kind of running continously, to the client, 
and this is all about allowing a server to resend a request to the client, 
then at some future point of time, the client decides to run a tool, so it might make a call tool request using a post request, 
and this request gets routed to the second MCP server, the second server, would then respond by setting up its own seperate Response, 

as the tool runs inside the server, 
imagine for  a second,it needs to make use of claude, and
try to use claude by creating a "create message request"
like sampling

sampling requests always always need to go through the Get SSE response, but here, that connection formed by completley seperate server, 
so we have to make "create message request" get to first server, and have server send the request through the "GET SSE Response" 
have the MCP CLIENT run claude 
and generate some text, send it back to first server, and then get that generated text back to the second server, 

but all this requires extra codes and infrastructure set up, adding complexity, 

to avoid above complexity , we can use 
stateless http transport

mcp=FastMCP("mcp-server", stateless_http =True,json_response=True)

when we set above flags as True, 
it means, 
we dont get a session id : server cant keep track of/send requests to  clients, 
disables: 
1. server to client requests, 
2. Sampling
3. Progress Reports
4. Subscriptions, (resources updates)

Post request responses arent streamed

Effects of no Session ID

1. GET SSE response cant be used anymore - the server cant figure out how to pair that response pathway with any incoming request.
2. without the GET SSE response, we cant use sampling, progress logging, subscriptions,
3. in Stateless mode, client initialization is no longer required.

MCP SERVER ----GET SSE RESPONSE---> MCP CLIENT
MCP CLIENT ----POST SSE RESPONSE ---->MCP Server


in stateless http mode, 
we dont have to make these two requests, 
initilaize requests
initialize notifications

so , it cuts down the amount of network traffic, the server receives, 
as we use stateless_http=True

and when we use json_response =True
then it means
post requests that u send down to the clients are not going to have streaming enabled, 
we get result in plain json 

as

Response : Success

Content-length: 90
Content-type: application/json
mcp-session-id : weewrwrwrwrwrwfzfzffa
server :uvicorn

{"jsonrpc":"2.0",
  "id":3,
  "result":{
  "content":[
   {"type":"text",
    "text" : "8"}
    ],
   "isError":false}}


both of below flags changes how server behaves, 
  
json_response =True 
stateless_http=True

use http streamable in production

-----
The stateless_http and json_response flags in MCP servers control fundamental aspects of how your server behaves.
 Understanding when and why to use them is crucial, especially if you're planning to scale your server or deploy it in production.

When You Need Stateless HTTP
Imagine you build an MCP server that becomes popular. Initially, you might have just a few clients connecting to a single server instance:



As your server grows, you might have thousands of clients trying to connect. Running a single server instance won't scale to handle all that traffic:



The typical solution is horizontal scaling - running multiple server instances behind a load balancer:



But here's where things get complicated. Remember that MCP clients need two separate connections:

A GET SSE connection for receiving server-to-client requests
POST requests for calling tools and receiving responses


With a load balancer, these requests might get routed to different server instances. If your tool needs to use Claude (through sampling), 
the server handling the POST request would need to coordinate with the server handling the GET SSE connection. This creates a complex 
coordination problem between servers.



How Stateless HTTP Solves This
Setting stateless_http=True eliminates this coordination problem, but with significant trade-offs:



When stateless HTTP is enabled:

Clients don't get session IDs - the server can't track individual clients
No server-to-client requests - the GET SSE pathway becomes unavailable
No sampling - can't use Claude or other AI models
No progress reports - can't send progress updates during long operations
No subscriptions - can't notify clients about resource updates
However, there's one benefit: client initialization is no longer required. Clients can make requests directly without the initial handshake process.



Understanding JSON Response
The json_response=True flag is simpler - it just disables streaming for POST request responses. 
Instead of getting multiple SSE messages as a tool executes, you get only the final result as plain JSON.

With streaming disabled:

No intermediate progress messages
No log statements during execution
Just the final tool result
When to Use These Flags
Use stateless HTTP when:

You need horizontal scaling with load balancers
You don't need server-to-client communication
Your tools don't require AI model sampling
You want to minimize connection overhead
Use JSON response when:

You don't need streaming responses
You prefer simpler, non-streaming HTTP responses
You're integrating with systems that expect plain JSON

Development vs Production
If you're developing locally with standard I/O transport but planning to deploy with HTTP transport, test with the same transport you'll use in 
production. The behavior differences between stateful and stateless modes can be significant, and it's better to catch any issues during development 
rather than after deployment.

These flags fundamentally change how your MCP server operates, so choose them based on your specific scaling and functionality requirements.

===========

github.com/orgs/modelcontextprotocol/discussions

modelcontextprotocol.io

=============

Your MCP server needs to use Claude to summarize data, but you don't want the server to handle API costs. What feature should you use?

 Sampling

---
Question 2:  Correct answer
Your MCP tool sends a "Call Tool Request" and expects to get back results. What type of message pattern is this?

 
 Request-result message




Question 3:  Correct answer
Your StreamableHTTP server needs to send progress updates to clients, but HTTP doesn't normally allow server-initiated requests. How does StreamableHTTP solve this?

 It creates Server-Sent Events (SSE) connections


Question 4:  Correct answer
A user asks Claude to "convert video.mp4" but Claude doesn't know where the file is located. What MCP feature helps solve this?

 Roots

Question 5:  Correct answer
You want simpler HTTP responses without streaming, just getting the final result as plain JSON. Which flag should you enable?

 json_response=True


Question 6:  Correct answer
You're developing an MCP server locally and want the simplest way to test communication between client and server on the same machine. Which transport should you use?

 Stdio transport


Question 7:  Correct answer
Which transport method requires both client and server to run on the same machine?


 Stdio transport

Question 8:  Correct answer
What are roots in MCP?

 A system that tells MCP servers what files/folders it can access

 
Question 9:  Correct answer
What is the correct sequence for MCP connection initialization?

 Initialize Request  Initialize Result  Initialized Notification

Question 10:  Correct answer

What is sampling in MCP?
A way for servers to access language models through connected MCP clients.

=============================

CLAUDE WITH AMAZON BEDROCK

Claude offers three distinct model families, each optimized for different priorities. All three models share Claude's core capabilities - they can handle text generation, coding, image analysis, and other tasks. The key difference is how they balance intelligence, speed, and cost.


Claude Opus
Opus delivers Claude's highest level of intelligence. It's designed for complex scenarios that require sophisticated reasoning and planning capabilities.

Opus excels at working independently on complex projects for extended periods. It can manage multi-step processes and navigate different requirements without much human intervention. The model supports reasoning, meaning it can provide quick responses for simple tasks or spend time thinking through more complex problems.

The trade-off is moderate latency and higher cost. You're paying more and waiting longer for that extra intelligence.

Claude Sonnet
Sonnet sits in the sweet spot of Claude's lineup, offering a balanced combination of intelligence, speed, and cost that works well for most practical applications.

What makes Sonnet particularly valuable is its strong coding ability combined with fast text generation. Many developers appreciate its ability to make precise edits to complex codebases without breaking existing functionality.

Claude Haiku
Haiku is Claude's fastest model, built specifically for applications where response time is critical. It's optimized for speed and cost efficiency rather than maximum intelligence.

One important limitation: Haiku doesn't support the reasoning capabilities that Opus and Sonnet offer. This makes it ideal for user-facing applications that need real-time interactions but less suitable for complex problem-solving tasks.

Choosing the Right Model

Model selection comes down to understanding the trade-offs between intelligence and cost/speed. Here's how to decide:

Choose Opus when intelligence is your top priority. If you have complex tasks requiring strong reasoning capabilities, you're choosing quality over speed and cost.
Choose Haiku when speed matters most. For real-time user interactions or high-volume processing where you need the fastest possible responses.
Choose Sonnet when you need balance. Most applications benefit from Sonnet's combination of intelligence, speed, and reasonable cost.
Using Multiple Models
Many teams don't stick to just one model. Instead, they use different models for different parts of the same application:

Haiku for user-facing interactions where speed is crucial
Sonnet for main business logic
Opus for complex tasks requiring deeper reasoning
This approach lets you optimize each part of your application for its specific requirements while managing overall costs and performance.

--------------------

When building applications with AI models, you need to understand the flow of data from user input to AI-generated response. Let's walk through how this works with AWS Bedrock and see what happens behind the scenes of a typical chat application.

How Chat Applications Work
Imagine you're building a web app with a simple chat interface. A user types "Define quantum computing" and clicks send. Here's what actually happens:



The user sees a clean interface, but there's a whole system working behind the scenes to generate that response.



The Request Flow
When a user submits text, here's the journey that message takes:



User submits their message through your web interface
Your server receives the request containing that text
Your server uses the Bedrock client to make a request to AWS Bedrock
The request includes the user message and a model ID (like Claude Haiku or Claude Sonnet)
The chosen model processes the request and generates text
AWS Bedrock sends back an assistant message containing the generated response
Your server forwards this response back to the user's browser

-----

making a request :

we need :
bedrock runtime client : we will make client using boto 3 python module
model id : id of the model hosted in bedrock that we want to invoke
user message: specially formatted object containing text,that we want to feed into model


## codes to create a client, get the model id, and create a user message

import boto3
client =boto3.client("bedrock-runtime", region_name="US West2")
model_id ="us.anthropic.claude-sonnet-4-20250514-v1:0"
user_message ={
   "role":"user", 
   "content":[
        { "text": "what is add"}
    ]
   }

response=client.converse(
modelID =model_id,
messages=[user_message]

response  , press enter

or

response["output"]["messaage"]]"context"][0]["text"]



but there is problem, not all regions have all the models hosted in it, 
so we can use inference profiles, 
inference profiles route requests to a region where ur chosen model is hosted.

cross region inference, select model, select inference profile id

2 types of messages:
user message: content we want to feed into the model., role ="user
Assistant message: content the model has produced., role="assistant"

a message can have below content
{ "role":"user",
  "content": [
  {
    "image": {
     "format" :"png",
     "source": {
      "bytes": "...."
    }
    }
    },
   { "text": "what is in image"}
   }

------------

Making your first API request to AWS Bedrock requires three essential components: a Bedrock Runtime Client to connect to the service, a Model ID to specify which model you want to run, and a User Message containing the text you want to feed into the model.

Setting Up the Bedrock Client
Start by creating a client using boto3 to connect to the Bedrock runtime service:

import boto3

client = boto3.client("bedrock-runtime", region_name="us-west-2")
Understanding Model IDs and Regional Availability
Here's where things get tricky. Not every model is available in every AWS region. If you try to run a model that doesn't exist in your chosen region, you'll get a cryptic error message saying the model doesn't exist.



For example, if Claude Sonnet is available in us-west-2 but you're making requests from us-east-1, your request will fail.



Using Inference Profiles
Inference profiles solve the regional availability problem by automatically routing your requests to a region where your chosen model is actually hosted.



Instead of tracking which models are in which regions, you can use an inference profile that knows the model is available in multiple regions like us-west-2 and us-east-2.



When you make a request using an inference profile, AWS automatically routes it to the correct region where your model exists, even if you're connecting from a different region.

To find inference profile IDs, go to the AWS Bedrock console and look under "Cross-region inference" rather than using the model ID from the main model catalog page.



Copy the inference profile ID for your chosen model.

Creating User Messages
User messages have a specific structure that might look overly complex at first, but there's a good reason for it:

user_message = {
    "role": "user",
    "content": [
        {"text": "What's 1+1?"}
    ]
}
The content is a list because a single message can contain different types of content - text, images, or other media types. This structure allows you to send multimodal requests.



Making the Request
Now you can make your API call using the converse method:

response = client.converse(
    modelId=model_id,
    messages=[user_message]
)
The response contains a lot of metadata, but to get just the generated text, you need to navigate through the response structure:

response["output"]["message"]["content"][0]["text"]
Understanding Message Types
There are two main message types you'll work with:

User messages - Content you want to feed into the model (role: "user")
Assistant messages - Content the model has produced (role: "assistant")


Both message types follow the same structure with a role and content list. This consistency makes it easy to build conversations by alternating between user and assistant messages.

The assistant message you get back from Bedrock follows the exact same format as your user message, just with a different role. This standardized structure makes it straightforward to chain multiple requests together for longer conversations.

----------

Mutli Turn conversations:
Bedrock and claude do not store any messages,
to have a conversation, u need tp 
a. manually maintain a list of messages in ur code and context of messages.
b. provide that list of messages with each follow up request.


Multi Turn conversations:

response = client.converse(
 modelId=model_id,
 messages=[{
   "role":"user",
   "content":[
      { "text" :"add 3 more"}
]
}]
}








   






